<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Alex Lu's Blog Feed</title>
        <link>https://op8867555.github.io</link>
        <description><![CDATA[Alex Lu's Blog Feed]]></description>
        <atom:link href="https://op8867555.github.io/feed.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Thu, 23 Aug 2018 00:00:00 UT</lastBuildDate>
        <item>
    <title>ONYX Boox Note 使用心得</title>
    <link>https://op8867555.github.io/posts/2018-07-15-boox-note.html</link>
    <description><![CDATA[<section>
<p>前一陣子(2018/07/07)買了 ONYX Boox Note 這台 10.3 吋的 e-ink reader 來讀書, 這邊紀錄目前使用心得。</p>
</section>
<section id="起因" class="level1">
<h1>起因</h1>
<p>因為 Kindle 6&quot; 的螢幕實在沒有很好讀 PDF，所以大多數的情況還是用電腦螢幕閱讀，但是後來越來越常覺得眼睛疲勞，於是就開始物色其他選項。</p>
<p>在稍微做了點功課之後，我選了文石的 Boox Note<span><label for="sn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="marginnote"> <img src="https://i.imgur.com/GlVMZ57.jpg" /><br />
<br />
</span></span>，主要的原因是：</p>
<ul>
<li><strong>10.3吋的螢幕</strong><br />
看很多人說看 PDF paper 沒有 10 吋不方便，而 13.3 吋的選項都太大太難帶了</li>
<li><strong>內建的 PDF reader 的功能</strong><br />
頁面裁切、可以用 TOC、有針對多欄論文的閱讀模式…等</li>
<li><strong>Android 作業系統</strong><br />
雖然是有點舊的 6.0，但是有很大的彈性來使用其他 app</li>
<li><strong>入手容易</strong><br />
相較於其他幾台 Sony DPT-CP1、Remarkable、還沒上市的 Good e-Reader 10.3，Boox Note 相對好入手，台灣有代理也方便售後服務。</li>
</ul>
</section>
<section id="使用狀況" class="level1">
<h1>使用狀況</h1>
<p>目前我用過的幾個使用情境是：</p>
<ul>
<li>看技術書籍/ paper</li>
<li>看漫畫/小說</li>
<li>聽演講、思考時做做筆記</li>
</ul>
<section id="pros" class="level2">
<h2>Pros</h2>
<ul>
<li><strong>電子紙</strong>，這個沒什麼好說的，對眼睛真的舒服很多</li>
<li><strong>PDF 的導覽功能</strong>
<ul>
<li><a href="https://i.imgur.com/4bH7XqN.gif">自動去除白邊</a>，使 A5 大小還可以直接閱讀一般的 paper</li>
<li><a href="https://i.imgur.com/SRoTBZd.gif">article mode</a>，用大字體也很方便閱讀</li>
</ul></li>
<li><strong>手寫筆記方便</strong></li>
<li><p><strong>可以用喜歡的 app</strong></p>
<ul>
<li>看漫畫用 Perfect Viewer</li>
<li>kindle 內容可以直接用 app 看</li>
</ul></li>
</ul>
</section>
<section id="cons" class="level2">
<h2>Cons</h2>
<ul>
<li>PDF Reader 不支援 logical page numbering<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">像是那些 i, ii 之類的頁碼 <img src="https://i.imgur.com/HKtLYVy.png" /><br />
<br />
</span></span>，查 index 時很不方便</li>
<li>PDF Reader 不支援切換 LTR 或是 RTL 翻頁模式（西文書常用左開、亞洲書多為右開）</li>
<li>PDF Reader 不支援 two-page view，也不支援同時多文件閱讀</li>
<li><strong>筆記功能薄弱</strong>，沒有圖層、不分顏色、不能複製貼上、不能整理頁面</li>
</ul>
</section>
</section>
<section id="結論" class="level1">
<h1>結論</h1>
<p>目前用了快兩個月了，整體來說是很方便的。不過以這個價位<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">16張小朋友<br />
<br />
</span></span>來說，這個產品的軟體實在不是很成熟，很多小細節都還有待加強，希望可以靠未來的更新來改進。</p>
</section>]]></description>
    <pubDate>Thu, 23 Aug 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-07-15-boox-note.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>Closed Nightmare 玩後感</title>
    <link>https://op8867555.github.io/posts/2018-08-03-closed-nightmare.html</link>
    <description><![CDATA[<section>
<p>這幾天在 Switch 的新聞看到這個遊戲的介紹，看到是難得的恐怖懸疑 AVG，查了一下也有港台版，便跑去買了一張來玩了…</p>
</section>
<section id="中文化" class="level1">
<h1>中文化</h1>
<p>玩起來沒什麼覺得有問題的地方，也沒遇到港台用語不一致的問題。但是美中不足的是，幕後花絮影片並沒有提供字幕，對於不懂日文的人來說樂趣相對得被減少了。</p>
<figure>
<img src="http://i.imgur.com/snnnNekh.jpg" alt="港台版只有提供繁體中文，而不像大部分的遊戲個人經驗，我手邊的遊戲有中文的通常都可以切日文  提供複數語言支援。" /><figcaption>港台版只有提供繁體中文，而不像大部分的遊戲<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">個人經驗，我手邊的遊戲有中文的通常都可以切日文<br />
<br />
</span></span>提供複數語言支援。</figcaption>
</figure>
<figure>
<img src="http://i.imgur.com/YzUJGzVh.jpg" alt="日版中的不同字體的部份也有保留" /><figcaption>日版中的不同字體的部份也有保留</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/Y2x99SBh.jpg" alt="背景中出現的文字也有翻譯" /><figcaption>背景中出現的文字也有翻譯</figcaption>
</figure>
</section>
<section id="遊戲性" class="level1">
<h1>遊戲性</h1>
<ul>
<li><strong>遊戲系統</strong>非常單純，主要就是：看影片、視覺小說、點擊解迷的三種模式在交叉換。影響遊戲走向的只有選項分支跟所持道具<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">雖然遊戲有章節跳躍的功能，但是所持道具不會跟著跳躍而繼承，前面章節漏拿的話，需要從該章開始從頭打到尾…<br />
<br />
</span></span>的不同。</li>
<li><strong>劇情</strong>本身幾乎是一條線，剩下的分歧通常都是直達 bad end 而已，沒有複雜條件的分支<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">玩到真結局心中還是有許多疑問，也有些沒有回收的伏筆，許多細節設定也都沒有解釋，說真的會讓人覺得意義不明。<br />
<br />
</span></span>。</li>
<li><strong>演出</strong>是真人實物拍攝，但是整個氣氛反而有點低成本電影的感覺，使得遊玩過程中充滿喜感。</li>
<li><strong>恐怖性</strong>實在很薄弱，主要都是以 jump scare 為主，我認為氣氛營造的並不會讓人感到恐怖、也沒什麼壓力。</li>
<li><strong>謎題難度</strong>整體上不難，大多數都可以輕鬆突破<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">少部份比較難的就是時間限制的拼圖跟沒按照特定順序就拿不到的必要道具等…<br />
<br />
</span></span>。</li>
<li><strong>隱藏要素</strong>不多：只有全破之後的結局清單、音樂鑑賞及幕後花絮<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">幕後花絮還算有趣，可以看到過程中很多沒有想到的拍攝手法。<br />
<br />
</span></span>；真結局的支線也只是多了一小段影片，沒有新的章節、解迷要素。</li>
<li><strong>遊戲時間</strong>不長：第一輪跑完的時間大概是 3~5 小時，跑真結局跟收集全要素的時間頂多也是 10 小時上下。以一片大作價格的作品來說，內容絕對不算多。</li>
</ul>
</section>
<section id="總結" class="level1">
<h1>總結</h1>
<ul>
<li>整體遊玩算比較輕鬆、沒壓力的日式AVG。</li>
<li>衝著恐怖的人來說，這片是會失望的程度。</li>
<li>劇情說不上很強，但是整體的遊玩體驗還算是好的。</li>
<li>內容很難對得起它的定價，等二手或是特價再買錢包比較不會痛 <span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">PS4版的玩家還可以拿白金，NS玩家如我就什麼都沒有留下了<br />
<br />
</span></span>。</li>
</ul>
</section>]]></description>
    <pubDate>Fri, 03 Aug 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-08-03-closed-nightmare.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>Facebook Prophet 模型簡介</title>
    <link>https://op8867555.github.io/posts/2018-05-29-facebook-prophet.html</link>
    <description><![CDATA[<section>
<p>去年 Facebook 公開了一個時間序列<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">時間序列是按照時間順序的資料點所構成，舉幾個常見的時間序列是股價、每小時氣溫、一個網站每分鐘的點擊數…等。<br />
<br />
</span></span>的預測模型 - <a href="https://facebook.github.io/prophet/">Prophet</a>，這篇文章簡介它的構造及運作原理。</p>
</section>
<section id="the-model" class="level1">
<h1>The Model</h1>
<section id="additional-model" class="level2">
<h2>Additional Model</h2>
<p>Prophet 用了一個很經典的加法模型來描述資料</p>
<p><span class="math display">\[ y(t) = g(t) + s(t) + h(t) + ε_t \]</span></p>
<p>在所有時間點 <span class="math inline">\(t\)</span> 的數值（<span class="math inline">\(y(t)\)</span>） 是由：</p>
<ul>
<li>趨勢的影響 <span class="math inline">\(g(t)\)</span>、</li>
<li>季節性的影響 <span class="math inline">\(s(t)\)</span>、</li>
<li>假日的影響 <span class="math inline">\(h(t)\)</span> 及</li>
<li>誤差 <span class="math inline">\(ε_t\)</span></li>
</ul>
<p>這四個成份所加總而來。</p>
</section>
<section id="trend" class="level2">
<h2>Trend</h2>
<p>Prophet 提供了兩種不同的趨勢函數供使用者使用。</p>
<ul>
<li><p>Linear 用於不會飽和的預測<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">比方說 Facebook 的使用者人數不會超過當時可以連網的總人口數，這個容量上界可能會隨著時間、市場變化而有所改變。<br />
<br />
</span></span></p>
<p><span class="math display">\[g(t) = k \cdot t + m\]</span></p></li>
<li><p>Logistic 用於會飽和的預測</p>
<p><span class="math display">\[g(t) = \frac{C(t)}{1+\exp(-k \cdot (t-m))}\]</span></p>
<p>其中 <span class="math inline">\(C(t)\)</span> 是不同時間點的「容量」上界。</p></li>
</ul>
<section id="change-points" class="level3">
<h3>Change Points</h3>
<p>除了單純的趨勢函數之外， Prophet 還引入了轉折點(change point)的想法，也就是讓趨勢函數在不同的時間區間內，能有不同的增長率 <span class="math inline">\(k\)</span>。</p>
<p><span><label for="sn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="marginnote"> <span class="math inline">\(\mathbf{1}(\text{expr})\)</span> 是 <span class="math inline">\(f(t) = \begin{cases} 1 &amp; \text{if expr is true} \\ 0 &amp; \text{otherwise} \end{cases}\)</span> 的一個簡寫方法，被稱為 indicator function.<br />
<br />
</span></span></p>
<p><span class="math display">\[k(t) = k_0 + \sum_{j} \mathbf{1}(t \leq t_j) \cdot δ_j\]</span></p>
<p>其中 <span class="math inline">\(t_j\)</span> 表示 <span class="math inline">\(j\)</span> 這個轉折點的時間，也就是一開始的基礎 <span class="math inline">\(k_0\)</span> 加上接下來的所有變化。</p>
<p>另外，調整了 <span class="math inline">\(k\)</span> 就得調整 <span class="math inline">\(m\)</span>，調整後的公式就是<a href="https://peerj.com/preprints/3190/">原文</a>中的 (3) 跟 (4)，這邊就不列出。</p>
</section>
</section>
<section id="seasonality" class="level2">
<h2>Seasonality</h2>
<p>Prophet 應用 Fourier series 來描述季節性<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">季節性是固定週期所帶來的影響，比方說：餐廳的來客數會隨的週一到週日有著類似的規律、通勤時間交通比較壅塞…等。<br />
<br />
</span></span></p>
<p><span class="math display">\[s(t) = \sum_{n}^{N} \left( a_n \cos \left(\frac{2\pi nt}{P}\right) +
b_n \sin\left(\frac{2\pi nt}{P} \right)\right)\]</span></p>
<p>其中 <span class="math inline">\(P\)</span> 是週期的長度（年的週期是 365.25 天，周的週期是 7 天…），透過控制 <span class="math inline">\(a_n, b_n\)</span> 參數，可以近似出任何一個周期函數。</p>
</section>
<section id="holiday" class="level2">
<h2>Holiday</h2>
<p>Prophet 假設每種假日 <span class="math inline">\(D_i\)</span> 都會帶來不同的影響。</p>
<p><span class="math display">\[ h(t) = \sum_{i}^{D} κ_i \cdot \mathbf{1}(t \in D_i) \]</span></p>
</section>
</section>
<section id="fitting-the-model" class="level1">
<h1>Fitting the model</h1>
<p>前面定義好了模型的結構，但是要怎麼樣挑選參數呢？ Prophet 應用了機率模型來描述這些參數，</p>
<p><span class="math display">\[\begin{gathered}
k_0, m &amp; \sim &amp;  \mathcal{N}(0,5) \\
δ_i &amp; \sim &amp;  Laplace(0, τ) \\
κ_i, a_i, b_i &amp; \sim &amp;  \mathcal{N}(0, σ) \\
ε_t &amp; \sim &amp;  \mathcal{N}(0, 0.5)\\
y_t &amp; \sim &amp; \mathcal{N}\left( g(t)+s(t)+h(t), ε_t \right) \\
\end{gathered}\]</span></p>
<p>接著再用 L-BFGS 演算法找出這些參數的最大後驗估計(Maximum a posterori estimation; MAP) 或是用 MCMC 來 inference 出每個參數的後驗機率。</p>
<p>找出這些參數之後，我們就可以拿來預測未來的數值。</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Prophet 提供了一個麻瓜也會用<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">不過也得先會操作 R 或是 Python<br />
<br />
</span></span>的預測模型：</p>
<ul>
<li>只需要想預測的資料就能動</li>
<li>可以允許空資料點（傳統的統計方法不行）</li>
<li>轉折點可以手動指定，也可以自動預測</li>
<li>可以考慮會飽和的情況</li>
<li><span class="math inline">\(τ, σ\)</span> 兩個 hyper-parameter 可以微調模型</li>
<li>因為是加法模型，結果能很簡單的解釋</li>
</ul>
<p>雖然沒辦法包含更深入的 feature（天氣狀況、特定事件），但是如上述優點，這麼模型還是可以在商業時間序列預測上達到不錯的成效。</p>
</section>]]></description>
    <pubDate>Tue, 29 May 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-05-29-facebook-prophet.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>[en] Notes on Thinkpad X1 Carbon 2017 + Arch Linux</title>
    <link>https://op8867555.github.io/posts/2017-06-11-X1C5-notes.html</link>
    <description><![CDATA[<section id="changelogs" class="level3">
<h3>changelogs</h3>
<ul>
<li>2018/04/18 Saoto’s trackpoint cap and fingerprinter status update.</li>
<li>2018/02/11 add a note for recall experience and thunderbolt 3 failures.</li>
<li>2018/01/12 update info.</li>
<li>2017/08/22 update kernel to 4.12; add a WWAN section.</li>
<li>2017/07/22 rewrite the post in English, remove some outdated information.</li>
</ul>
</section>
<section id="quick-summary" class="level1">
<h1>Quick Summary</h1>
<p>I’m running Arch Linux (kernel 4.15) on a Thinkpad X1 Carbon 5th Gen (X1C5).</p>
<p>Most stuff works out of box,</p>
<ul>
<li>wifi</li>
<li>backlit keyboard</li>
<li>LEDs on keyboard (caps lock/mute/mic mute)</li>
<li>TrackPoint - slow speed but works.</li>
<li>TrackPad - not so precise but works.</li>
<li>USB-C - USB-C to HDMI / USB-C Hub<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">I wrote a <a href="/posts/2017-07-21-adam-usb-c-dock.html">blog post</a> in Traditional Chinese 🇹🇼 about the hub.<br />
<br />
</span></span> works.</li>
<li>Bluetooth - works with a FC30 game controller.</li>
<li>WWAN - works with ModemManager.</li>
<li>mini-Ethernet - works.</li>
<li>micro SD reader</li>
</ul>
<p>Things <em>partially</em> working,</p>
<ul>
<li>Fingerprint reader</li>
</ul>
<p>I haven’t tried,</p>
<ul>
<li>Thunderbolt 3 <span><label for="sn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="marginnote"> I confused usb-c with TB3, it turns out I don’t have TB3 device.<br />
<br />
</span></span></li>
<li>hibernation - I don’t setup a swap partition.</li>
</ul>
</section>
<section id="hardware" class="level1">
<h1>Hardware</h1>
<section id="trackpoint" class="level2">
<h2>TrackPoint</h2>
<section id="alps-or-elantech" class="level3">
<h3>ALPS or Elantech?</h3>
<p>X1C5 comes with two versions<span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote"><a href="https://patchwork.kernel.org/patch/9768231/" class="uri">https://patchwork.kernel.org/patch/9768231/</a><br />
<br />
</span></span> of TrackPoint: ALPS and Elantech. I confirmed mine has an Elantech one using <a href="http://pcsupport.lenovo.com/tw/zh/products/laptops-and-netbooks/thinkpad-x-series-laptops/thinkpad-x1-carbon-type-20hr-20hq/downloads/ds122148">TrackPointDetect.exe</a>.</p>
<pre><code>TouchPad-Flash Utility
2017-06-08 20:42:27
Command Line: &quot;C:\Users\alexlu\Desktop\FW_Updater_1.0.0.9\TrackPointDetect.exe&quot;
OS: 64-bit
Pst Vendor : Elan
[ERROR]: This TrackPoint is Elan (ID:03)</code></pre>
</section>
<section id="cursor-speed-trackpoint-cap" class="level3">
<h3>Cursor Speed &amp; TrackPoint Cap</h3>
<p>I used to take the plastic adapter<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote"><img src="https://i.imgur.com/qyS29cDl.jpg" title="fig:" alt="The plastic adapter from previous Thinkpad" /><br />
<br />
</span></span> from my previous laptop - a Thinkpad E130 and trimmed a Soft Rim Cap to fit “Super Low Profile”. <span><label for="sn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="marginnote"> <img src="https://i.imgur.com/cxoY1AO.jpg" /> IMHO, a soft rim is <em>much better</em> than a soft dome.<br />
<br />
</span></span></p>
<p>I have switched to a 3d-printed rubber caps made by <a href="https://twitter.com/saoto28">Saoto</a>, it works really great.</p>
<p><img src="https://i.imgur.com/zJF2hz2.jpg" /></p>
<p>The cursor speed and acceleration gets much better.</p>
</section>
</section>
<section id="trackpad" class="level2">
<h2>TrackPad</h2>
<p>It works with libinput and pressure-based palm detection are functional. But it is still unresponsive to small movements for current release. <span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote"><a href="https://bugs.freedesktop.org/show_bug.cgi?id=98839" class="uri">https://bugs.freedesktop.org/show_bug.cgi?id=98839</a><br />
<br />
</span></span></p>
<p><span><label for="sn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="marginnote"> There is a different device according to <a href="https://www.reddit.com/r/thinkpad/comments/6wimb8/doing_some_research_on_x1c5_touchpad_issues/">this thread</a>.<br />
<br />
</span></span></p>
<p><code>dmesg</code> output:</p>
<pre><code>rmi4_smbus 0-002c: registering SMbus-connected sensor
rmi4_f01 rmi4-00.fn01: found RMI device, manufacturer: Synaptics, product: TM3289-002, fw id: 2492434</code></pre>
</section>
<section id="fingerprint-reader" class="level2">
<h2>Fingerprint Reader</h2>
<p>X1C5 have a Validity Sensor 138a:0097. There is no fully-working driver for it at this moment.</p>
<p>There is an open source reverse engineered driver working in progress<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote"><a href="https://github.com/nmikhailov/Validity90" class="uri">https://github.com/nmikhailov/Validity90</a><br />
<br />
</span></span><span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote"><a href="https://bugs.freedesktop.org/show_bug.cgi?id=94536" class="uri">https://bugs.freedesktop.org/show_bug.cgi?id=94536</a><br />
<br />
</span></span>.</p>
<section id="partially-working-driver" class="level3">
<h3>Partially-Working Driver</h3>
<ul>
<li>Nikita Mikhailov(<span class="citation" data-cites="nmikhailov">@nmikhailov</span>) reverse-engineered how to use 0097 to verify fingerprint, but fingerprints need to be enrolled in Windows first.</li>
<li>Marco Trevisan(<span class="citation" data-cites="3v1n0">@3v1n0</span>) wrote a match-on-host driver<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote"><a href="https://github.com/3v1n0/libfprint" class="uri">https://github.com/3v1n0/libfprint</a><br />
<br />
</span></span> for 0090.</li>
<li>Aleksandr Saraikin(<span class="citation" data-cites="hrenod">@hrenod</span>) made a modification<span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle"/><span class="sidenote"><a href="https://github.com/hrenod/libfprint" class="uri">https://github.com/hrenod/libfprint</a><br />
<br />
</span></span> for 0097 based on <span class="citation" data-cites="3v1n0">@3v1n0</span>’s work.</li>
</ul>
<p><span class="citation" data-cites="hrenod">@hrenod</span>’s fork make it possible to use 0097 to authenticate now. One just need to:</p>
<ol type="1">
<li>initialize the device and enroll fingerprints in Windows,</li>
<li>install <code>fprintd</code> and <span class="citation" data-cites="hrenod">@hrenod</span>’s <code>libfprint</code>,</li>
<li>do a fake enroll (ex. <code>fprintd-enroll</code>),</li>
<li>set up PAM rules.</li>
</ol>
</section>
</section>
<section id="wwan" class="level2">
<h2>WWAN</h2>
<p>X1C5 (WWAN) comes with a Sierra EM7455 LTE Modem (1199:9079).</p>
<p>It works out of box using latest ModemManager.</p>
</section>
<section id="thunderbolt-3" class="level2">
<h2>Thunderbolt 3</h2>
<section id="firmware-failures" class="level3">
<h3>Firmware Failures</h3>
<p>My X1C5’s usb-c port dies sometime after I plug-in and out my usb-c hub, it was unable to charge and connect any devices, and it sometimes just “recovered” itself. I initially guess it is a hardware failure and sent the laptop once to repair center, but the technician can’t reproduce the issue.</p>
<p>Later I found there is a new <a href="https://pcsupport.lenovo.com/us/zh/products/laptops-and-netbooks/thinkpad-x-series-laptops/thinkpad-x1-carbon-type-20hr-20hq/downloads/ds120382">Thunderbolt 3 firmware update</a>, which says it:</p>
<ul>
<li>Fixed an issue where system might not be charged after repeating AC plug-in and out.</li>
</ul>
<p>It is not happen again after firmware updated so far. Intel WTF.</p>
</section>
</section>
<section id="recall-for-loose-screws" class="level2">
<h2>Recall for loose screws</h2>
<p>Lenovo recently(2018/02/06) recalls X1C5 made before 2017/11 and Mine is unfortunately affected. I went to nearest Lenovo Service Center(around 4km) and it took 15~20 minutes for the technician to check. There is no loose screws at last.</p>
</section>
</section>
<section id="software" class="level1">
<h1>Software</h1>
<p>I’m using i3-gaps without a desktop environment.</p>
<section id="hardware-video-acceleration" class="level2">
<h2>Hardware Video Acceleration</h2>
<p>It is possible to use the integrated graphic card to decode/encode videos, and this helps power-saving a lot.</p>
<p><span><label for="sn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-12" class="margin-toggle"/><span class="marginnote"> See this <a href="https://wiki.archlinux.org/index.php/Hardware_video_acceleration">article</a> on ArchWiki for more info.<br />
<br />
</span></span></p>
<p>There are two spec/implement/API: VAAPI (by Intel) and VDPAU (by Nvidia). Since most software supports VAAPI, I just need to install <code>libva-intel-driver</code> pacakge.</p>
</section>
<section id="libva-intel-driver" class="level2">
<h2>libva-intel-driver</h2>
<p>Current release version has a bug<span><label for="sn-13" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-13" class="margin-toggle"/><span class="sidenote"><a href="https://github.com/01org/intel-vaapi-driver/issues/297" class="uri">https://github.com/01org/intel-vaapi-driver/issues/297</a><br />
<br />
</span></span> when decoding VP9 videos, <code>libva-intel-driver-git</code> package should fixes that.</p>
</section>
<section id="chromium-vaapi" class="level2">
<h2>chromium-vaapi</h2>
<p>There is a VAAPI implementation for Chromium OS, and there is also a patch<span><label for="sn-14" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-14" class="margin-toggle"/><span class="sidenote"><a href="https://chromium-review.googlesource.com/c/chromium/src/+/532294" class="uri">https://chromium-review.googlesource.com/c/chromium/src/+/532294</a><br />
<br />
</span></span> to enable that on Linux. The <code>chromium-vaapi-bin</code> package on AUR provides prebuilt binary, or you can build <code>chromium-vaapi</code> by yourself (It takes about 6 hrs first time, you can use ccache to speed up if you want to build it more than one time).</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://gist.github.com/gdamjan/141bb0def5f80257fae2b233ec16f3c2">gdamjan’s Note</a></li>
<li><a href="http://fredrik.wendt.se/2017/04/26/lenovo-thinkpad-x1-carbon-5th-generation/">Fredrik’s Note</a></li>
<li><a href="https://forums.lenovo.com/t5/forums/v3_1/forumtopicpage/board-id/tp02_en/thread-id/75464/page/29">discussion about TrackPad</a></li>
</ul>
</section>
<section id="outdated" class="level1">
<h1>Outdated</h1>
<p>These issues/tricks/workarounds are mostly outdated(linux kernel ≤ 4.12). Maybe I should just remove this section someday…</p>
<section id="trackpoint-1" class="level2">
<h2>TrackPoint</h2>
<section id="issues" class="level3">
<h3>Issues</h3>
<ul>
<li><p>Recognized as a “PS/2 Generic Mouse”<br />
⇒ No palm-detection during trackpoint use</p></li>
<li><p>Since kernel driver doesn’t support the new trackpoint, both of sensitivity and speed aren’t adjustable.<br />
⇒ The default speed/sensitivity is too slow to use.</p></li>
</ul>
</section>
<section id="mark-it-as-a-pointing-stick" class="level3">
<h3>Mark it as a pointing stick</h3>
<p>I found I can making libinput treat the “PS/2 Generic Mouse” as a trackpoint using a custom udev hwdb entry.</p>
<p><em>Note: this should not be used anymore once trackpoint gets kernel driver supports</em></p>
<pre><code>evdev:name:PS/2 Generic Mouse:dmi:*svnLENOVO*:pvrThinkPadX1Carbon5th*
  ID_INPUT_POINTINGSTICK=1</code></pre>
</section>
<section id="better-trackpoint-acceleration" class="level3">
<h3>Better TrackPoint Acceleration</h3>
<p>There is a <a href="https://bugs.freedesktop.org/show_bug.cgi?id=91369#c48">patch</a> which solves speed issue.</p>
<p>I get my trackpoint much usable by setting up:</p>
<ul>
<li><code>LIBINPUT_ATTR_TRACKPOINT_RANGE=10</code> in hwdb</li>
<li><code>xinput set-prop &quot;PS/2 Generic Mouse&quot; &quot;libinput Accel Speed&quot; -0.25</code> in my .xinitrc</li>
</ul>
</section>
</section>
<section id="trackpad-1" class="level2">
<h2>TrackPad</h2>
<p>The trackpad is <ins>able to</ins> use rmi4 over smbus for now(4.12).</p>
<p>Turn it on by creating a <code>psmouse.conf</code> in <code>/etc/modprobe.d/</code></p>
<pre><code>options psmouse synaptics_intertouch=1</code></pre>
<section id="palm-detection" class="level3">
<h3>Palm Detection</h3>
<ul>
<li>pressure-based palm-detection works great (libinput ≥ 1.8)</li>
<li>palm-detection during trackpoint use requires <a href="#mark-it-as-a-pointing-stick">mark it as a pointing stick</a>.</li>
</ul>
</section>
</section>
<section id="new-hotkeys" class="level2">
<h2>New Hotkeys</h2>
<p>Before linux-4.12, You needs <a href="https://patchwork.kernel.org/patch/9596129/">these</a> <a href="https://patchwork.kernel.org/patch/9596131/">patches</a> to get these buttons works.</p>
</section>
</section>]]></description>
    <pubDate>Wed, 18 Apr 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2017-06-11-X1C5-notes.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>T-Digest 概念簡介</title>
    <link>https://op8867555.github.io/posts/2018-04-09-tdigest.html</link>
    <description><![CDATA[<section>
<p>百分位數是敘述性統計中一個很好用的工具，像是</p>
<ul>
<li>用中位數來代表台灣人的所得</li>
<li>用 75 百分位數來找出學測前標的成績</li>
<li>用 99.99 百分位數來找出特別的顧客；或是找出異常的伺服器連線等。</li>
</ul>
<p>但是因為計算相對複雜，在資料量過大時往往無法計算， T-Digest 是一個近似大量資料中百分位數的演算法，這篇文章簡單介紹其想法。</p>
</section>
<section id="concepts" class="level1">
<h1>Concepts</h1>
<p>假設我們今天有很多數字，舉例來說這邊有 500 個 -30 ~ 30 間的數字</p>
<p><span><label for="sn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="marginnote"> 這種圖稱為 Rug plot, 其中的每一條直線代表一個數字，其 x 軸是它的值。<br />
<br />
</span></span> <img src="/images/tdigest/numbers.svg" data-caption="1" /></p>
<p>我們可以用 Probabilistic Density Function(PDF) 的形式來表示：</p>
<p><span><label for="sn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="marginnote"> 機率密度函數，<span class="math inline">\(PDF(x)\)</span> 越高表示 <span class="math inline">\(x\)</span> 發生的機率越高，其面積加總起來是 100%<br />
<br />
</span></span></p>
<p><img src="/images/tdigest/dist.svg" /></p>
<p>75 百分位數就是面積佔 75% 時的 x 座標：</p>
<p><img src="/images/tdigest/dist_75.svg" /></p>
<section id="centroid" class="level2">
<h2>Centroid</h2>
<p>但是這樣要知道 500 個數字的順序才能計算，這在數字非常多的時候是不容易的。取而代之的是，試著將用很少的數字來代替 500個數字，怎麼做呢？</p>
<p><strong>將鄰近的數字們用兩個數字代表： 平均值跟個數</strong> <span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">原始的論文中是用總和，但是兩者意義相同，實作上為了效率都會選用平均值。<br />
<br />
</span></span></p>
<p>這樣的一組平均值跟個數被稱為 <em>Centroid</em> ，我們可以用好幾個 centroid 來代替整個 PDF, <strong>T-Digest 就是在做這件事</strong>。</p>
<p><img src="/images/tdigest/centroids.svg" /></p>
<p>當然可以自由決定要用幾個 centroid 來代表</p>
<p><img src="/images/tdigest/centroids2.svg" /></p>
<p>計算百分位數時就是只需要從這些 centroids 中找出對應位置的項目來做內插法。</p>
<p><img src="/images/tdigest/centroids_pdf.svg" /></p>
</section>
<section id="clustering" class="level2">
<h2>Clustering</h2>
<p>將很多數字用一個或多個 centroids 代表的動作稱為 <em>clustering</em>. 我們知道當一個 centroid 代表的數字越多，流失的資訊也就越多，也就越不精確，因此<strong>怎麼 clustering 就是影響成效的一個重點</strong>。</p>
<p>T-Digest 的作法是控制各個 centroid 的大小，在需要精確的部份用更多 centroids 來表達。為此論文提出了一個公式<span><label for="sn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="marginnote"> <img src="/images/tdigest/k_q_function.svg" alt="k(q, δ) 的函數圖形" /><br />
<br />
</span></span>來定義這個行為：</p>
<p><span class="math display">\[k(q, δ) = δ\left(\frac{sin^{-1}(2q - 1)}{π} + \frac{1}{2}\right)\]</span></p>
<p>其中 <span class="math inline">\(q\)</span> 是 quantile, <span class="math inline">\(δ\)</span> 是壓縮率參數，而 <span class="math inline">\(k(q, δ)\)</span> 表示 <span class="math inline">\(q\)</span> 應該要用第幾個 centroid 表達。可以從圖中看到在靠近 <span class="math inline">\(q = 0\)</span> 跟 <span class="math inline">\(q = 1\)</span> 的時候，<span class="math inline">\(k\)</span> 的變動很大，也就是說我們用比較多的 centroids 來表示。 為什麼這樣設計呢？</p>
<p>這是因為大部分計算百分位數時，真正關心的通常是 1、0.1、99、99.9、99.99…等很極端的百分位數，相較之下中間部份不那麼在意其精確程度了。</p>
</section>
<section id="merge-and-the-rest" class="level2">
<h2>Merge and the rest …</h2>
<p>除了透過 clustering 建構 T-Digest 之外，論文中也給了合併兩個 T-Digest 的方法。這帶來了一個很大的好處：當資料點太多時，我們可以分組計算 T-Digest 然後在合併起來，這讓平行分散計算變得可能<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">很重要的一點是 T-Digest 的 merge 需要滿足結合律才能安全的用在平行分散的系統上。但很可惜的是， T-Digest 是近似演算法，所以沒有真正的滿足結合律。雖然如此，仍然可以在一定的誤差之下滿足這些條件，也使得實務上還是可行。<br />
<br />
</span></span>，也就是 Divide and Conquer.</p>
<p>論文中剩下的部份除了討論成效之外，還給出了幾個實用的演算法：</p>
<ul>
<li>Trimmed Mean, 一個 quantile range 之間的平均值</li>
<li>Cumulative Density Function(CDF)</li>
<li>Inverse CDF</li>
</ul>
</section>
</section>
<section id="reference" class="level1">
<h1>Reference</h1>
<ul>
<li><a href="https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf">原始論文</a></li>
<li><a href="https://blog.bcmeng.com/post/tdigest.html">TDigest 算法原理</a></li>
<li><a href="https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest">Percentile and Quantile Estimation of Big Data: The t-Digest</a></li>
</ul>
</section>]]></description>
    <pubDate>Thu, 12 Apr 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-04-09-tdigest.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>到底什麼是 Bayesian</title>
    <link>https://op8867555.github.io/posts/2018-03-09-bayes.html</link>
    <description><![CDATA[<section>
<p>最近在公司內部的讀書會跟同事一起讀了這本「Probabilistic Programming and Bayesian Methods for Hackers」，看了很久覺得充滿許多疑惑，沒有辦法把概念跟概念連結起來，所以我在這邊試著以寫這篇文章的方式來對我自己解釋。</p>
<p>如果有發現文章敘述有誤，或是有更好的敘述方式，請告訴我一聲！</p>
</section>
<section id="什麼是-bayes-theorem" class="level2">
<h2>什麼是 Bayes’ Theorem</h2>
<p>Bayes’ Theorem 來自於條件機率的公式： <span class="math display">\[
P(A|B) = \frac{ \color{red}P(B|A) \color{black}\cdot \color{blue}P(A)}{\color{green}P(B)}
\]</span></p>
<ul>
<li>我們有兩個事件<span class="math inline">\(A\)</span>、<span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(\color{blue}P(A)\)</span>、<span class="math inline">\(\color{green}P(B)\)</span> 是 <span class="math inline">\(A\)</span> 跟 <span class="math inline">\(B\)</span> 獨自發生的機率</li>
<li><span class="math inline">\(P(A|B)\)</span> 及 <span class="math inline">\(\color{red}P(B|A)\)</span> 分別是已知 <span class="math inline">\(B\)</span> 之後 <span class="math inline">\(A\)</span> 發生，以及反過來的機率，又被稱為 likelihood</li>
</ul>
</section>
<section id="怎麼應用-bayes-theorem" class="level2">
<h2>怎麼應用 Bayes’ Theorem</h2>
<p>網路上有很多例子：<a href="https://en.wikipedia.org/wiki/Bayes%27_theorem#Drug_testing">檢測藥品反應</a>、<a href="https://brohrer.github.io/how_bayesian_inference_works.html">猜測一個人的性別</a>、<a href="http://www.greenteapress.com/thinkbayes/html/thinkbayes004.html#sec27">找出骰子點數應該來自幾面骰</a>，類似的例子很多，就是將兩個機率擺在一起，然後用觀察到的證據來找出問題的答案。</p>
<p>我就偷懶一下直接借用維基百科的藥品檢測的例子：</p>
<ul>
<li>假設有 0.5% 的人使用了這個藥品（<span class="math inline">\(P(\text{User}) = 0.005\)</span>）</li>
<li>而藥物測試的準確率是 99% （<span class="math inline">\(P(\text{+} | \text{User}) = 0.99\)</span>）</li>
</ul>
<p>那麼一個測出藥物反應的人真的有使用此藥物的機率有多高？</p>
<p><span class="math display">\[
\begin{aligned}
P({\text{User}}\mid {\text{+}})&amp;= {\frac {P({\text{+}}\mid {\text{User}})P({\text{User}})}{P(+)}}\\
&amp;={\frac {P({\text{+}}\mid {\text{User}})P({\text{User}})}{P({\text{+}}\mid {\text{User}})P({\text{User}})+P({\text{+}}\mid {\text{Non-user}})P({\text{Non-user}})}}\\[8pt]
&amp;={\frac {0.99\times 0.005}{0.99\times 0.005+0.01\times 0.995}}\\[8pt]
&amp;\approx 33.2\%
\end{aligned}
\]</span></p>
<!-- 這邊用 [Monty Hall Problem][monty-hall-wiki][^monty-hall-prob] 來舉例： -->
<!--   * 我們用 $P(A), P(B), P(C)$ 來分別表示大獎汽車在門A, 門B, 門C後面的機率 -->
<!--   * 我們先選了一扇門，假設選了門A -->
<!--   * $P(D)$ 來表示主持人 Monty 打開了**門B** 而且後面沒有車的機率 -->
<!-- 目標是求出 $P(A|D), P(B|D), P(C|D)$，因此我們必須先知道 likelihood： -->
<!--   * $P(D|A)$ 如果大獎在門A後面，主持人會從 B 跟 C 中隨便選一個打開，所以機率是 50%。 -->
<!--   * $P(D|B)$ 如果大獎在門B後面，主持人壓根兒不會打開它，所以機率為 0%。 -->
<!--   * $P(D|C)$如果大獎在門C後面，主持人打開門B後，裡面絕對沒有車，所以機率皆為 100%。 -->
<!-- 然後列出 -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- P(D|A) P(A) &= \frac{1}{2} \times \frac{1}{3} &= \frac{1}{6} \\ -->
<!-- P(D|B) P(B) &= 0 \times \frac{1}{3} &= 0 \\ -->
<!-- P(D|C) P(C) &= 1 \times \frac{1}{3} &= \frac{1}{3} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- 因為 $P(\mathit{H}|D) \propto P(D|\mathit{H}) P(\mathit{H}),\, \mathit{H} = A, B, C$， -->
<!-- 我們不必計算 $P(D)$就可以得出： 換到門C中獎的機率是維持選門A的兩倍！ -->
<!--
我這邊借用同事在讀書會舉的例子： 一枚連續擲出正面 10 次的硬幣是否公正，
用 $FairCoin$ 表示公正硬幣值出了正面，$H10$ 來表示一枚硬幣連續擲出 10 次正面。
在這個例子中，$P(FairCoin) = 0.5$ 就是先驗機率，而我們要找出
$$
P(FairCoin | H10) = \frac{P(H10 | FairCoin)P(FairCoin)}{P(H10)}
$$
-->
</section>
<section id="bayesian-inference-for-parameter-estimation" class="level2">
<h2>Bayesian Inference for Parameter Estimation</h2>
<p>跟前面的 Bayes’ Theorem一樣，但是這次將目標改變了一下： 用證據<span class="math inline">\(E\)</span>（training data）來找出模型參數<span class="math inline">\(θ\)</span> 的機率分佈</p>
<p><span class="math display">\[\begin{darray}{rlc}
P(θ|E) &amp;=&amp; \frac{ \color{red}P(E|θ) \color{black}\cdot \color{blue}P(θ)}{\color{green}P(E)} \\[1em]
&amp;=&amp; \frac{ \color{red}P(E|θ) \color{black}\cdot \color{blue}P(θ)}{\color{green}\int_θ P(E|θ&#39;) \cdot P(θ&#39;)\ dθ&#39;}
\end{darray}\]</span></p>
<ul>
<li><span class="math inline">\(\color{blue}P(θ)\)</span> 稱為先驗機率，也就是對於某事物的猜測。</li>
<li><span class="math inline">\(P(θ|E)\)</span> 稱為後驗機率，也就是觀察到 證據E 之後 H 的條件機率。</li>
<li><span class="math inline">\(\color{red}P(E|θ)\)</span> 稱為 likelihood function，有時會寫成 <span class="math inline">\(L(θ|E)\)</span>。</li>
<li><span class="math inline">\(\color{green}P(E)\)</span> 稱為 marginal likelihood，或是證據 E 事件所發生的機率</li>
</ul>
</section>
<section id="怎麼進行-bayesian-inference" class="level2">
<h2>怎麼進行 Bayesian Inference？</h2>
<p>跟前面一樣，建模者需要指定好：</p>
<ul>
<li>likelihood function，也就是描述資料是怎麼產生的函數，</li>
<li>prior，描述我們對目標參數的先驗知識。</li>
</ul>
<p>我在 Stack Overflow <span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote"><a href="https://stackoverflow.com/questions/375913/how-can-i-profile-c-code-running-in-linux">How can I profile C++ code running in Linux?</a><br />
<br />
</span></span> 上面看到了一個對 programmer 很有趣（也很實用）的應用：應用貝葉斯來做 profiling<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">Profiling 在軟體開發上指的是分析程式的執行狀況，以找出時間或是記憶體花費較多的片段來進行改進。<br />
<br />
</span></span>！</p>
<p>用這個作法只需要用 debugger 執行程式的時候，任意中斷個數次，然後觀察中斷時的 call stack，找看看裡面有沒有反覆出現的 function call。</p>
<p>假設我們觀察到了一個很常出現的函數，它在中斷 10 次中出現了 7 次，我們想知道它出現在 call stack 的比率 <span class="math inline">\(R\)</span>，也就是找出：</p>
<p><span><label for="sn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="marginnote">{-} <span class="math inline">\(T\)</span> 表示出現在 call stack 的次數， <span class="math inline">\(F\)</span> 表示沒有出現的次數 <br />
<br />
</span></span></p>
<p><span class="math display">\[
P(R | T=7, F=3) = \frac{P(T=7, F=3 | R) P(R)}{P(T=7, F=3)}
\]</span></p>
<p>為了化簡計算的複雜度，原文將 <span class="math inline">\(R\)</span> 分成 10 種可能 <span class="math inline">\(0.1, 0.2, \cdots, 1.0\)</span>。<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">這是為了舉例方便，實際上就得乖乖的去算積分了…<br />
<br />
</span></span> 我們不知道這個函數實際上的出現比率，所以我們用 Uniform Distribution 作為 Prior。</p>
<p><span class="math display">\[P(R=r) = 0.1, \, r = 0.1, ..., 1.0\]</span></p>
<p>因為 <span class="math inline">\(R\)</span> 是離散的，我們可以直接算出每一個 <span class="math inline">\(P(T=7, F=3 | R=r)\)</span> 的數值，剛好就是 Binomial Distribution：</p>
<p><span class="math display">\[P(T=t, F=f | R=r) = {t+f \choose t}\,r^{t}\,(1-r)^{f}\]</span></p>
<p><span><label for="sn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="marginnote"> <img src="/images/bayes-pmf.svg" /><br />
<br />
</span></span> 求出 <span class="math inline">\(P(R | T=7, F=3)\)</span> 之後，可以畫成圖來檢視、找出MAP、或是期望值。從右圖可以看到，後驗機率分佈的最高點落於 0.7，我們可以比較兩者的面積：</p>
<p><span class="math display">\[\begin{aligned}
  P(R | T=7, F=3, r = 0.7) \approx 30\% \\
  P(R | r = 0.7) \approx 10\%
  \end{aligned}\]</span></p>
<p>也就是在觀察到了證據之後，<span class="math inline">\(r = 0.7\)</span> 的機率從 10% 更新到 30% 了！</p>
</section>
<section id="什麼是-likelihood-function" class="level2">
<h2>什麼是 likelihood function？</h2>
<p>單看 likelihood function <span class="math inline">\(P(E|θ)\)</span> 的話，我覺得很奇怪，什麼是觀察到先驗H後證據發生的機率？</p>
<p>在 google 了好一段時間之後，我認為比較好的解釋方法是， likelihood 是「取得 證據B 來自於某個先驗 A 的機率」的函數 <span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">Think Bayes 的 <a href="http://www.greenteapress.com/thinkbayes/html/thinkbayes002.html#sec11">The diachronic interpretation</a><br />
<br />
</span></span><span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote"><a href="http://yenchic-blog.logdown.com/posts/182922-likelihood-function-myth">likelihood function迷思</a><br />
<br />
</span></span>，換句話說，就是描述證據（資料）是如何被產生的。</p>
<p>一個簡單的例子是，投擲 <span class="math inline">\(N\)</span> 次正面機率為 <span class="math inline">\(p\)</span> 的硬幣，觀察到的資料會是「<span class="math inline">\(N\)</span>次 裡面有 <span class="math inline">\(t\)</span> 次正面」的證據， likelihood function 這時會訂成 <span class="math display">\[P(E|θ) = L(\underbrace{p}_{\text{參數}\, θ} | \underbrace{N, t}_{\text{證據} \, E}) = {N \choose t} {p}^t {(1 - p)}^{N - t}\]</span></p>
</section>
<section id="frequentist-vs.bayesian" class="level2">
<h2>Frequentist vs. Bayesian</h2>
<p>統計上有兩個流派：</p>
<ul>
<li>Frequentist 認為機率就是頻率，而建模就是去最大化 likelihood</li>
<li>Bayesian 使用主觀的先驗知識來進行「預測」</li>
</ul>
<p>兩者在資料量夠大的情況會得到一樣的答案，但是 Bayesian 能在資料量不夠多時，搭配先驗知識，做出更好的預測。</p>
</section>
<section id="跟-sampling-的關係" class="level2">
<h2>跟 Sampling 的關係</h2>
<p>找出後驗機率分佈的時候，計算 Marginal likelihood 是一件困難的事<span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote"><a href="https://www.youtube.com/watch?v=EHqU9LE9tg8">Bayesian posterior sampling - YouTube</a><br />
<br />
</span></span><span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote"><a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf">15.097: Probabilistic Modeling and Bayesian Analysis</a><br />
<br />
</span></span>，少數狀況下可以利用挑選適當的共軛先驗來化簡計算，但是大多數的情況下是辦不到的。</p>
<p>但是因為在後驗機率分佈的時候，常常不需要知道準確(exact)的分佈，於是可以用從後驗機率分佈抽樣來取代。</p>
<!-- ## 為什麼一定要算出 marginal likelihood？ -->
<!-- 換個問法，既然 $P(θ | E) \propto L(θ | E)$，為什麼不直接用 likelihood 來代替？ -->
</section>
<section id="怎麼從後驗機率分佈中抽樣" class="level2">
<h2>怎麼從後驗機率分佈中抽樣？</h2>
<p>細節我雖沒有深入了解，但是大概是利用 MCMC 或是 Metropolis 演算法來做，可以參考：</p>
<ul>
<li>Probabilistic Programming and Bayesian Methods for Hackers 書中的 Ch.4 有簡單介紹爬山法的概念</li>
<li><a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/">MCMC sampling for dummies</a> 介紹如何手寫一個後驗機率分佈的 sampler</li>
</ul>
</section>
<section id="probabilistic-programming" class="level2">
<h2>Probabilistic Programming?</h2>
<p>Probabilistic Programming 是一種用軟體建立機率模型跟利用它們進行 inference 的方法<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">出自<a href="http://hakaru-dev.github.io/intro/probprog/">Hakaru 的官網</a><br />
<br />
</span></span>，實作上可能是一門獨立的新語言(DSL) 或是以 library 的方式嵌於某個 General-Purpose 的語言裡面。不論是 Frequentist 或是 Bayesian Inference 都可以用這個方法實作，但是比較常看到 Bayesian Inference 使用。</p>
<p>PyMC3 就是一個實作 Probabilistic Programming 的 Python library，舉例來說，前面的 profiling 問題可以用 PyMC3 表達成：</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> pymc3 <span class="im">as</span> pm</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="co"># prior</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">    r <span class="op">=</span> pm.Uniform(<span class="st">&#39;r&#39;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    <span class="co"># likelihood</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    res <span class="op">=</span> pm.Binomial(<span class="st">&#39;res&#39;</span>, <span class="dv">10</span>, r, observed<span class="op">=</span><span class="dv">7</span>)</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    <span class="co"># sampling from posterior</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    trace <span class="op">=</span> pm.sample(<span class="dv">25000</span>)</a></code></pre></div>
</section>
<section id="小結" class="level2">
<h2>小結</h2>
<p>雖然在寫這篇文章的過程中，解決了我心中不少的疑惑，但是仍然還是有許多似懂非懂之處，如果有新發現的話會再更新這篇文章。</p>
</section>]]></description>
    <pubDate>Fri, 30 Mar 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-03-09-bayes.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>機器如何計算微分/偏微分（下）</title>
    <link>https://op8867555.github.io/posts/2018-01-11-autodiff-reverse-mode.html</link>
    <description><![CDATA[<section>
<p><a href="/posts/2017-12-17-autodiff.html">前一篇文章</a>的最後介紹了 Forward-Mode Automatic Differential 這個應用 Chain Rule 的技巧，這一篇要來介紹一個反過來的版本 Reverse-Mode Automatic Differential。</p>
</section>
<section id="recap-forward-mode-ad" class="level1">
<h1>Recap: Forward-Mode AD</h1>
<p>計算 Foward-Mode AD 的時候，假設要求 <span class="math inline">\(f(x, y) = sin(x) + x × y\)</span> 在 <span class="math inline">\((3, 1)\)</span> 對 <span class="math inline">\(x\)</span> 的偏微分 <span class="math inline">\(\frac{\partial Y}{\partial x}\)</span>，我們會像這樣由上到下計算：</p>
<p><span><label for="sn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="marginnote"> <a href="https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/ForwardAccumulationAutomaticDifferentiation.png/512px-ForwardAccumulationAutomaticDifferentiation.png" /></a> Forward-Mode AD 在 <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia</a> 上的示意圖<br />
<br />
</span></span></p>
<p><span class="math display">\[\begin{array}{cl}
\frac{∂}{∂ x} x&amp;= 1\\[0.5em]
\frac{∂}{∂ x} sin(x) &amp;= cos(x) \\[0.5em]
\frac{∂}{∂ x} x \cdot y  &amp;= 1\\[0.5em]
\frac{∂}{∂ x} sin(x) + x \cdot y  &amp;= \frac{∂ sin(x)}{∂ x} +
\frac{∂ x \cdot y}{∂ x} = cos(x) + 1
\end{array}\]</span></p>
<p>也就是如果今天有個 <span class="math inline">\(f\)</span>，而將 <span class="math inline">\(x_i\)</span> 以外的輸入都是為常數的話，得到 <span class="math inline">\(f_{x_i} = g_1 \circ g_2 \circ \cdots \circ g_n\)</span>，</p>
<ol type="1">
<li>從 <span class="math inline">\(\frac{\partial x_i}{\partial x_i} = 1\)</span>，並將其他變數 <span class="math inline">\(x_j\)</span> 視為常數，</li>
<li>照著變數<span class="math inline">\(x_i\)</span> 的計算過程計算 <span class="math inline">\(g(x_i)\)</span> 的偏微分（這個例子的第一個 <span class="math inline">\(g\)</span> 是 <span class="math inline">\(sin\)</span> 或是 <span class="math inline">\(x \cdot y\)</span>）<span><label for="sn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="marginnote"> 我們可以將 <span class="math inline">\(\cdot\)</span> 看成 <span class="math inline">\(mul(x, y)\)</span>，而因為 <span class="math inline">\(y\)</span> 被視為常數，所以又可以看成一個單變數函數 <span class="math inline">\(mul_{1}(x) = 1\)</span>。<br />
<br />
</span></span></li>
<li>最後求得 <span class="math inline">\(\frac{\partial}{\partial x_i}f(..., x_i, ...)\)</span></li>
</ol>
<p>如果要計算每個變數 <span class="math inline">\(x_i\)</span> 的偏微分的話，我們就必須重複這樣的動作 <span class="math inline">\(N\)</span> 次，而計算梯度時就會遇到這個困境。</p>
</section>
<section id="reverse-mode-ad" class="level1">
<h1>Reverse-Mode AD</h1>
<p>Reverse-Mode AD 則是反過來計算：</p>
<p><span><label for="sn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="marginnote"> <a href="https://commons.wikimedia.org/wiki/File%3AReverseaccumulationAD.png"><img src="https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png" /></a> Rerverse-Mode AD 在 <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia</a> 上的示意圖<br />
<br />
</span></span></p>
<p><span class="math display">\[\begin{array}{cll}
\frac{∂ Y}{∂ Y} &amp;= \frac{∂ Y}{∂ sin(x) + x \cdot y} &amp;= 1 \\[0.5em]
\frac{∂ Y}{∂ sin(x)} &amp;=
\frac{∂ Y}{∂ sin(x) + x \cdot y}\frac{∂ sin(x) + x \cdot y}{∂ sin(x)} &amp;=
1 \cdot 1
\\[0.5em]
\frac{∂ Y}{∂ x \cdot y} &amp;=
\frac{∂ Y}{∂ sin(x) + x \cdot y}\frac{∂ sin(x) + x \cdot y}{∂ x \cdot y} &amp;=
1 \cdot 1
\\[0.5em]
\frac{∂ Y}{∂ x} &amp;=
\frac{∂ Y}{∂ sin(x)}\frac{∂ sin(x)}{∂ x} +
\frac{∂ Y}{∂ x \cdot y}\frac{∂ x \cdot y}{∂ x} &amp;=
1 \cdot cos(x) + 1 \cdot y
\\[0.5em]
\frac{∂ Y}{∂ y} &amp;=
\frac{∂ Y}{∂ x \cdot y}\frac{∂ x \cdot y}{∂ y} &amp;=
1 \cdot x
\end{array}\]</span></p>
<p>Forward-Mode AD 從 <span class="math inline">\(\frac{∂ \color{red}x}{∂ x} = 1\)</span> 開始<strong>組合</strong>，到 <span class="math inline">\(\frac{∂ \color{red}Y}{∂ x}\)</span> 結束； Reverse-Mode AD 從 <span class="math inline">\(\frac{∂ Y}{∂ \color{red}Y} = 1\)</span> 開始<strong>分解</strong>，到 <span class="math inline">\(\frac{∂ Y}{∂ \color{red}x}\)</span> 結束。</p>
<section id="back-propagation" class="level2">
<h2>Back Propagation</h2>
<p><span><label for="sn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="marginnote"> <a href="/images/autodiff-neuron.svg"><img src="/images/autodiff-neuron.svg" /></a> 類神經的神經元示意圖<br />
<br />
</span></span></p>
<p>所以這是如何跟類神經網路關聯接一起的？一個簡單的類神經網路<span><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">為了簡化，我省去了 Bias term <span class="math inline">\(b^{(l)}\)</span>。<br />
<br />
</span></span>可以這樣表示<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">我這邊使用 <span class="math inline">\(a\)</span> 表示 summation, <span class="math inline">\(z\)</span> 表示 activation 的輸出，有些文章會採用反過來的表示方式。<br />
<br />
</span></span>：</p>
<p><span class="math display">\[\begin{aligned}
a_i^{(1)} &amp;= \sum_j w^{(1)}_{ij} \cdot x_i
&amp; z_i^{(1)} &amp;= f(a_i^{(1)}) \\
a_i^{(2)} &amp;= \sum_j w^{(2)}_{ij} \cdot z_j^{(1)}
&amp; z_1^{(2)} &amp;= f(a_i^{(2)}) \\
&amp;\cdots &amp; &amp; \cdots \\
a_i^{(l)} &amp;= \sum_j w^{(l)}_{ij} \cdot z_j^{(l-1)}
 &amp;z_i^{(l)} &amp;= f(a_i^{(l)})
\end{aligned}\]</span></p>
<p>接著我們會定義 cost function (或稱為 loss function)，比方說 Mean Squared Error：</p>
<p><span class="math display">\[C(Y, \hat Y) = \frac{1}{n} \sum_i^n \left(Y_i - \hat{Y}_i\right)^2\]</span></p>
<p>類神經的 Forward Propagation 就是帶入資料 <span class="math inline">\(X\)</span> 跟權重 <span class="math inline">\(w\)</span> 計算出預測值 <span class="math inline">\(z_i^{(l)}\)</span>，而 Back Propagation 就是利用 cost 來找出一組更好 <span class="math inline">\(w_{ij}\)</span> 來使的 <span class="math inline">\(C\)</span> 更小。</p>
<p>怎麼做？從 <span class="math inline">\(\frac{∂ C}{∂ \color{red}C} = 1\)</span> 開始、 <span class="math inline">\(\frac{∂ C}{∂ \color{red}z_i^{(l)}}, \frac{∂ C}{∂ \color{red}a_i^{(l)}}, \frac{∂ C}{∂ \color{red}w_{ij}^{(l)}}, \frac{∂ C}{∂ \color{red}z_i^{(l-1)}} ...\frac{∂ C}{∂ \color{red}a_i^{(1)}}\)</span>，最後算到 <span class="math inline">\(\frac{∂ C}{∂ \color{red}w_{ij}^{(1)}}\)</span>，求出每一個 <span class="math inline">\(\frac{∂ C}{∂ w_{ij}}\)</span> 之後我們就可以利用 Gradient Descent 法來找出更好的 <span class="math inline">\(w_{ij}\)</span></p>
<p><span class="math display">\[w_{ij} \leftarrow w_{ij} - \eta \frac{∂ C}{∂ w_{ij}}\]</span></p>
<p>注意到了嗎？這跟前面所敘述的 Reverse-Mode AD 是<strong>一樣的計算過程</strong>，這代表我們可以利用它來取代<span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">Back Propagation 跟 AD 都應用了 chain rule 來計算，不同的是使用 AD 的技巧後不再需要推導公式。<br />
<br />
</span></span>手動推導的 Back Propagation 演算法。</p>
</section>
<section id="為什麼類神經要用-reverse-mode-ad" class="level2">
<h2>為什麼類神經要用 Reverse-Mode AD?</h2>
<p>Jacobian matrix 是一個 <span class="math inline">\(f: ℝ^n \rightarrow ℝ^m\)</span> 的所有偏導數所組合而成的矩陣：</p>
<p><span class="math display">\[\mathbf {J} =
\begin{bmatrix}
\frac {\partial f_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac {\partial f_{1}}{\partial x_{n}}\\[0.5em]
\vdots &amp;\ddots &amp;\vdots \\[0.5em]
\frac {\partial f_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac {\partial f_{m}}{\partial x_{n}}
\end{bmatrix}\]</span></p>
<p>Reverse-Mode AD 就是一次算一個 row，Forward-Mode AD 則是一次算一個 column。由於類神經網路是一個 <span class="math inline">\(n \gg m\)</span> 的函數，用 Reverse-Mode AD 有效率了不少。</p>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<p>Reverse-Mode AD 的實作比起 Forward-Mode AD 複雜了點，先回來看看前面的<span class="math inline">\(\frac{∂ Y}{∂ x}\)</span>：</p>
<p><span class="math display">\[\frac{∂ Y}{∂ x} =
\textcolor{blue}{\frac{∂ Y}{∂ sin(x)}}
\textcolor{green}{\frac{∂ sin(x)}{∂ x}} +
\textcolor{blue}{\frac{∂ Y}{∂ x \cdot y}}
\textcolor{green}{\frac{∂ x \cdot y}{∂ x}}\]</span></p>
<p>可以觀察到：</p>
<ol type="1">
<li><span class="math inline">\(\color{blue}\text{藍色}\)</span>的部份跟計算過程是<strong>相反順序的</strong><span><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote">以計算 <span class="math inline">\(Y = (f \circ g \circ h) (x)\)</span> 的微分 <span class="math inline">\(\frac{∂ Y}{∂ x}\)</span> 為例，我們需要從後面到前面地求出 <span class="math inline">\(\frac{∂ Y}{∂ f}, \frac{∂ Y}{∂ g}, \frac{∂ Y}{∂ h}\)</span>，<br />
<br />
</span></span></li>
<li><span class="math inline">\(\color{green}\text{綠色}\)</span>的部份跟計算過程是<strong>相同順序的</strong><span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote">繼續上面的例子，計算綠色的部份需要從前面到後面的求出 <span class="math inline">\(\frac{∂ h(x)}{∂ x}, \frac{∂ g(h(x))}{∂ h(x)}, \frac{∂ f(g(h(x)))}{∂ g(h(x))}\)</span>。<br />
<br />
</span></span></li>
<li>我們需要追蹤一個變數被在哪些地方，上面 <span class="math inline">\(Y = sin(x) + x \cdot y\)</span> 的例子 <span class="math inline">\(x\)</span> 就被用 <span class="math inline">\(sin(x)\)</span> 跟 <span class="math inline">\(x \cdot y\)</span> 兩處。</li>
</ol>
<section id="wengert-tape" class="level2">
<h2>Wengert Tape</h2>
<p>其中一個作法是使用 Wengert Tape ，這個資料結構追蹤順向計算的過程，Reverse-Mode AD 時則是會反過來計算。</p>
<p>為了簡化，這邊將計算的過程推廣成三種形式： 變數、單變數函數、雙變數函數。<span><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">這邊的實作參考自 Edward Kmett 所寫的 <a href="https://github.com/ekmett/ad"><code>ad</code></a> 自動微分函式庫。<br />
<br />
</span></span><span><label for="sn-11" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-11" class="margin-toggle"/><span class="sidenote">注意：為了方便實作跟解釋，我的實作有稍微變化過。<br />
<br />
</span></span></p>
<ul>
<li><code>x</code> （或是 <code>y</code>） 表示這個計算的輸入節點 <span class="math inline">\(x, y\)</span>（反向的達到了前述第 3 點）</li>
<li><code>dx</code> （或是 <code>dy</code>） 表示輸入對這個計算的影響 <span class="math inline">\(\frac{∂f}{∂x}, \frac{∂f}{∂y}\)</span>，也就是前述第 2 點的<span class="math inline">\(\color{green}\text{綠色}\)</span>部份。</li>
<li><code>ss</code> 表示這個計算對於整個計算的影響 <span class="math inline">\(\frac{∂ Y}{∂ f}\)</span>，也就是前述第 1 點的<span class="math inline">\(\color{blue}\text{藍色}\)</span>部份。</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> collections <span class="im">import</span> namedtuple</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">Var <span class="op">=</span> namedtuple(<span class="st">&#39;Var&#39;</span>, [<span class="st">&#39;ss&#39;</span>])</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">Un  <span class="op">=</span> namedtuple(<span class="st">&#39;Un&#39;</span>,  [<span class="st">&#39;x&#39;</span>, <span class="st">&#39;dx&#39;</span>, <span class="st">&#39;ss&#39;</span>])</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">Bin <span class="op">=</span> namedtuple(<span class="st">&#39;Bin&#39;</span>, [<span class="st">&#39;x&#39;</span>, <span class="st">&#39;y&#39;</span>, <span class="st">&#39;dx&#39;</span>, <span class="st">&#39;dy&#39;</span>, <span class="st">&#39;ss&#39;</span>])</a></code></pre></div>
<p>這樣就可以下面的這段程式來表達 <span class="math inline">\(sin(x) + x \cdot y \bigg\vert_{x = 3, y = 1}\)</span> 的順向計算過程<span><label for="sn-12" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-12" class="margin-toggle"/><span class="sidenote">為了簡化，我省去了當下計算的輸出數值<code>v</code>(<span class="math inline">\(f(x)\)</span>)<br />
<br />
</span></span></p>
<p><span><label for="sn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-13" class="margin-toggle"/><span class="marginnote"> 這邊我小小的作弊了一下，直接使用 <code>Dict</code> 來方便更新 <code>ss</code> 的值。<br />
<br />
</span></span></p>
<p><span><label for="sn-14" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-14" class="margin-toggle"/><span class="marginnote"> <a href="/images/autodiff-forward-ad.svg"><img src="/images/autodiff-forward-ad.svg" /></a> 這張圖表示順向計算時所累計的數值<br />
<br />
</span></span></p>
<p><span><label for="sn-15" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-15" class="margin-toggle"/><span class="marginnote"> <a href="/images/autodiff-tape.svg"><img src="/images/autodiff-tape.svg" /></a> 這張圖表示節點之間的相依關係<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="im">from</span> math <span class="im">import</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">def</span> box(v<span class="op">=</span><span class="dv">0</span>):</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    <span class="cf">return</span> {<span class="st">&#39;value&#39;</span>: v}</a>
<a class="sourceLine" id="cb2-4" data-line-number="4"></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">x <span class="op">=</span> Var(box())</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">y <span class="op">=</span> Var(box())</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">sin_x <span class="op">=</span> Un(x, cos(<span class="dv">1</span>), box())</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">x_times_y <span class="op">=</span> Bin(x, y, <span class="dv">1</span>, <span class="dv">3</span>, box())</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">Y <span class="op">=</span> Bin(sin_x, x_times_y, <span class="dv">1</span>, <span class="dv">1</span>, box())</a></code></pre></div>
<p>Wengert Tape 就是紀錄輸入變數及這一連續的計算過程。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1">tape <span class="op">=</span> [x, y, sin_x, x_times_y, Y]</a></code></pre></div>
<p>當計算 Reverse-Mode AD時，我們會將 tape 反過來累計：<span><label for="sn-16" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-16" class="margin-toggle"/><span class="marginnote">{-} 注意到這邊的實作已經在前面建構這些節點的同時，將順向計算的過程給嵌進去了。<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1">Y.ss[<span class="st">&#39;value&#39;</span>] <span class="op">=</span> <span class="dv">1</span> <span class="co"># 從 ∂Y/∂Y = 1 開始</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="cf">for</span> cell <span class="kw">in</span> <span class="bu">reversed</span>(tape):</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">    <span class="cf">if</span> <span class="bu">isinstance</span>(cell, Bin):</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">        cell.x.ss[<span class="st">&#39;value&#39;</span>] <span class="op">+=</span> cell.ss[<span class="st">&#39;value&#39;</span>] <span class="op">*</span> cell.dx</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">        cell.y.ss[<span class="st">&#39;value&#39;</span>] <span class="op">+=</span> cell.ss[<span class="st">&#39;value&#39;</span>] <span class="op">*</span> cell.dy</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    <span class="cf">elif</span> <span class="bu">isinstance</span>(cell, Un):</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">        cell.x.ss[<span class="st">&#39;value&#39;</span>] <span class="op">+=</span> cell.ss[<span class="st">&#39;value&#39;</span>] <span class="op">*</span> cell.dx</a></code></pre></div>
<p>Reverse Mode AD 就像這樣在一個 pass 就能算出所有輸入的偏導數</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> x.ss[<span class="st">&#39;value&#39;</span>], y.ss[<span class="st">&#39;value&#39;</span>]</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">(<span class="fl">0.010007503399554585</span>, <span class="dv">3</span>)</a></code></pre></div>
<p>剩下的就是實作一個易用的界面給使用者了，也就是只需要指定順向計算的過程。</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> var(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> y <span class="op">=</span> var(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span> Y <span class="op">=</span> sin(x) <span class="op">+</span> x <span class="op">*</span> y</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="op">&gt;&gt;&gt;</span> Y.value</a>
<a class="sourceLine" id="cb6-5" data-line-number="5"><span class="fl">3.1411200080598674</span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6"><span class="op">&gt;&gt;&gt;</span> Y.tape</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">Tape([<span class="op">&lt;</span>Var <span class="bu">object</span> at <span class="bn">0x7f84dbfed198</span><span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb6-8" data-line-number="8"> <span class="op">&lt;</span>Unary <span class="bu">object</span> at <span class="bn">0x7f84e6968c50</span><span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb6-9" data-line-number="9"> <span class="op">&lt;</span>Var <span class="bu">object</span> at <span class="bn">0x7f84e3e567b8</span><span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb6-10" data-line-number="10"> <span class="op">&lt;</span>Binary <span class="bu">object</span> at <span class="bn">0x7f84e0dff4e0</span><span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb6-11" data-line-number="11"> <span class="op">&lt;</span>Binary <span class="bu">object</span> at <span class="bn">0x7f84dbfed630</span><span class="op">&gt;</span>])</a>
<a class="sourceLine" id="cb6-12" data-line-number="12"><span class="op">&gt;&gt;&gt;</span> Y.backprop()</a>
<a class="sourceLine" id="cb6-13" data-line-number="13"><span class="op">&gt;&gt;&gt;</span> x.sensitivity</a>
<a class="sourceLine" id="cb6-14" data-line-number="14"><span class="fl">0.010007503399554585</span></a></code></pre></div>
<section id="其他變化" class="level3">
<h3>其他變化</h3>
<ul>
<li>另外一個做法是將所有節點表示成一個有向無環圖（DAG; Directed Acyclic Graph），接著做 Topological sort 來計算出順向跟反向的計算順序。</li>
<li>可以選擇將記住計算的函數及輸入輸出，而非偏導數的數值 <span><label for="sn-17" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-17" class="margin-toggle"/><span class="sidenote">如果我沒弄錯的話， tensorflow 就是以 DAG 表示，用 DFS 來找出計算順序，並採用只儲存函數本身跟輸入輸出的做法。<br />
<br />
</span></span>。</li>
<li>可以在變數節點記下名稱，方便事後觀察。</li>
</ul>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>這篇文章介紹了 Reverse-Mode AD 以及實作的方法，學會這些細節雖然沒有讓我更會訓練類神經網路，但也是更了解這些黑盒子底下到底在做些什麼。</p>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="example-softmax-regression" class="level2">
<h2>Example: Softmax Regression</h2>
<p><span><label for="sn-18" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-18" class="margin-toggle"/><span class="marginnote">{-} 我的「玩具版」 Reverse-Mode AD 實作函數庫放在<a href="https://gist.github.com/op8867555/59d246a54188fe0d282656fe83e84a65">這邊</a>。<br />
<br />
</span></span></p>
<p>這次以 ML 常用的 iris 資料集作為範例，建構一個 Softmax Regression 來分類這些花朵。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="im">from</span> reverse_ad <span class="im">import</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5">iris <span class="op">=</span> load_iris()</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">X <span class="op">=</span> iris[<span class="st">&#39;data&#39;</span>]</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">Y <span class="op">=</span> iris[<span class="st">&#39;target&#39;</span>]</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(X, Y)</a></code></pre></div>
<p>這邊使用 Softmax Regression 做分類，用 Cross Entropy 做為 loss function。<span><label for="sn-19" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-19" class="margin-toggle"/><span class="marginnote"> <span class="math display">\[\begin{gathered}
  Softmax(X_i) = \frac{exp(X_i)}{\sum_k exp(X_i)} \\
  f(X; W, B) = Softmax(W \times X + B) \\
  Cost(Y, \hat Y) = \frac{-1}{|Y|}\sum_{y} \sum_{k} y_k \log(\hat{y}_k)
  \end{gathered}\]</span><br />
<br />
</span></span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">def</span> softmax(z):</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">    z_exp <span class="op">=</span> [exp(i) <span class="cf">for</span> i <span class="kw">in</span> z]</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">    sum_z_exp <span class="op">=</span> <span class="bu">sum</span>(z_exp)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">    <span class="cf">return</span> [i <span class="op">/</span> sum_z_exp <span class="cf">for</span> i <span class="kw">in</span> z_exp]</a>
<a class="sourceLine" id="cb8-5" data-line-number="5"></a>
<a class="sourceLine" id="cb8-6" data-line-number="6"><span class="kw">def</span> reg(xs, w, b):</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">    <span class="cf">return</span> softmax([<span class="bu">sum</span>(w[i][j] <span class="op">*</span> x <span class="cf">for</span> j, x <span class="kw">in</span> <span class="bu">enumerate</span>(xs)) <span class="op">+</span> b[i]</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">                    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)])</a>
<a class="sourceLine" id="cb8-9" data-line-number="9"></a>
<a class="sourceLine" id="cb8-10" data-line-number="10"><span class="kw">def</span> categorical_cross_entropy(Y, Y_):</a>
<a class="sourceLine" id="cb8-11" data-line-number="11">    loss <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">sum</span>(k <span class="op">*</span> log(k_) <span class="cf">for</span> k, k_ <span class="kw">in</span> <span class="bu">zip</span>(y, y_)) <span class="cf">for</span> y, y_ <span class="kw">in</span> <span class="bu">zip</span>(Y, Y_))</a>
<a class="sourceLine" id="cb8-12" data-line-number="12">    <span class="cf">return</span> <span class="dv">-1</span> <span class="op">*</span> loss <span class="op">/</span> <span class="bu">len</span> (Y_)</a></code></pre></div>
<p>用 Gradient Descent 來找到更好的 <span class="math inline">\(W\)</span> 跟 <span class="math inline">\(B\)</span></p>
<p><span><label for="sn-20" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-20" class="margin-toggle"/><span class="marginnote"> 這邊需要先對標籤做 one-hot encoding 才會有跟公式輸出一致的維度。 <br />
<br />
</span></span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">Y_encoded <span class="op">=</span> OneHotEncoder().fit_transform(Y_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).todense().tolist()</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">w <span class="op">=</span> [[var(<span class="dv">0</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)]</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">b <span class="op">=</span> [var(<span class="dv">0</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)]</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">lr <span class="op">=</span> <span class="fl">0.1</span></a></code></pre></div>
<p><span><label for="sn-21" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-21" class="margin-toggle"/><span class="marginnote"> 使用 Reverse Mode AD 就不再需要每一個參數都再計算一次。<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">    Y_predicted <span class="op">=</span> [reg(x, w<span class="op">=</span>w, b<span class="op">=</span>b) <span class="cf">for</span> x <span class="kw">in</span> X_train]</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">    loss <span class="op">=</span> categorical_cross_entropy(Y_encoded, Y_predicted)</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">    loss.backprop()</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">        b[i] <span class="op">-=</span> lr <span class="op">*</span> b[i].sensitivity</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">            w[i][j] <span class="op">-=</span> lr <span class="op">*</span> w[i][j].sensitivity</a>
<a class="sourceLine" id="cb10-9" data-line-number="9">    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">        accuracy <span class="op">=</span> accuracy_score(Y_test, [tolabel(reg(x, w<span class="op">=</span>w, b<span class="op">=</span>b)) <span class="cf">for</span> x <span class="kw">in</span> X_test])</a>
<a class="sourceLine" id="cb10-11" data-line-number="11">        <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="sc">{</span>epoch<span class="sc">: 5d}</span><span class="ss"> loss </span><span class="sc">{</span>loss<span class="sc">.</span>value<span class="sc">}</span><span class="ss"> acc </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">&#39;</span>)</a></code></pre></div>
<p>跑起來成果像是這樣：</p>
<pre><code>    0 loss 1.098612288668108   acc 0.34210526315789475
   10 loss 0.84323472659183    acc 0.6578947368421053
   20 loss 0.716723873894274   acc 0.6578947368421053
   30 loss 0.6493242141034026  acc 0.6578947368421053
   40 loss 0.6030807982006535  acc 0.6578947368421053
   50 loss 0.5661515656844388  acc 0.6578947368421053
   60 loss 0.5340659822896819  acc 0.6578947368421053
   70 loss 0.5047568856139167  acc 0.6578947368421053
   80 loss 0.4771475569375817  acc 0.6578947368421053
   90 loss 0.45064476873213055 acc 0.6578947368421053
  100 loss 0.42493012219412674 acc 0.6578947368421053
  110 loss 0.39987107522910936 acc 0.7105263157894737
  120 loss 0.37549205547113373 acc 0.7105263157894737
  130 loss 0.3519881981460512  acc 0.7368421052631579
  140 loss 0.3297816399879364  acc 0.7631578947368421
  150 loss 0.30960905639139913 acc 0.868421052631579
  160 loss 0.2925264903210591  acc 0.9210526315789473</code></pre>
<!--

example :: Diagram B
example = nodes # applyAll [connectOutside a b | (a, b) <- names]
                # padX 1.2 # padY 1.1
                # centerXY
    where names = concat [ zip inputsNames weightsNames
                         , zip weightsNames (repeat "sum")
                         , [("sum", "act"), ("act", "out")]
                         ]

nodes = foldl1 (|||)  [ vsep 1 inputs # centerY
                      , strutX 3
                      , vsep 1 weights # centerY
                      , strutX 3
                      , sum_ # centerY
                      , strutX 1.5
                      , text "a" # translateY 0.5
                      , strutX 1.5
                      , act # centerY
                      , strutX 1
                      , out
                      ]
withText = replicate 5 (text "×" # translateY 0.5)

inputsNames = map (("input_"++) . show) [1..5]
weightsNames = map (("weight_"++) . show) [1..5]
inputs  = (\x -> mconcat [ text "z⁽ˡ⁻¹⁾"
                         , circle 1.25 # named x
                         , text "×" # translate (r2 (3, 0.5))
                         ]) <$> inputsNames
weights = map (\x -> text "w" <> circle 1.25 # named x) weightsNames
sum_    = text "Σ" <> circle 1.25 # named "sum"
act     = text "f" <> circle 1.25 # named "act"
out     = text "z⁽ˡ⁾"
-->
</section>
</section>]]></description>
    <pubDate>Sun, 28 Jan 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-01-11-autodiff-reverse-mode.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>機器如何計算微分/偏微分（上）</title>
    <link>https://op8867555.github.io/posts/2017-12-17-autodiff.html</link>
    <description><![CDATA[<section>
<p>提到近年來熱門的類神經網路模型，反傳導(back propagation)是一個相當重要的過程，其中應用的就是計算梯度（gradient，就是每個參數的偏微分）修正神經參數來達到「學習」的效果，類神經網路的結構越來越複雜，使得人工推導梯度公式越來越不可行，如何應用電腦計算微分變成了一個重要的問題。</p>
</section>
<div class="fullwidth">
<figure>
<img src="https://i.imgur.com/R5QzZY4.png" alt="這張圖簡單地解釋了計算微分的幾種作法。摘自“Automatic Differentiation of Algorithms for Machine Learning”" /><figcaption>這張圖簡單地解釋了計算微分的幾種作法。摘自“Automatic Differentiation of Algorithms for Machine Learning”</figcaption>
</figure>
</div>
<section id="numerical-differential" class="level1">
<h1>Numerical Differential</h1>
<p><span><label for="sn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="marginnote">{-} <img src="https://upload.wikimedia.org/wikipedia/commons/1/18/Derivative.svg" /> 數值微分， By Olivier Cleynen (Own work) [CC0], via Wikimedia Commons<br />
<br />
</span></span></p>
<p>數值微分就是用 <span class="math inline">\(\frac{f(x + h) - f(x)}{h}\)</span> 跟一個很小的 <span class="math inline">\(h\)</span> 來<strong>近似</strong> <span class="math inline">\(\lim_{h \rightarrow 0}\frac{f(x + h) - f(x)}{h}\)</span>，實作上也只是計算用不同的參數來計算函數 <span class="math inline">\(f\)</span> 的差異，但是 <span class="math inline">\(f\)</span> 的計算成本可能不低，而且很容易遇到浮點數計算的 round-off error 跟 truncating error，所以實務上並不實用。</p>
<section id="實作" class="level2">
<h2>實作</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> math <span class="im">import</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">def</span> diff(f, x, epilson<span class="op">=</span><span class="fl">0.00001</span>):</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="cf">return</span> (f(x<span class="op">+</span>epilson) <span class="op">-</span> f(x))<span class="op">/</span>epilson</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">diff(sin, pi) <span class="co"># -0.9999999999898844</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">def</span> f(x, y):</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">    <span class="cf">return</span> sin(x) <span class="op">+</span> x <span class="op">*</span> y</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co"># 計算偏微分就是把不關心的參數視為常數</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="im">from</span> functools <span class="im">import</span> partial</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">diff(partial(f, y<span class="op">=</span><span class="dv">2</span>), <span class="dv">3</span>) <span class="co"># 1.0100067978413563 (df(2,3)dx)</span></a></code></pre></div>
</section>
</section>
<section id="symbolic-differential" class="level1">
<h1>Symbolic Differential</h1>
<p>符號微分是符號計算 (Symbolic Computing) 的一個經典例子：不去計算數值，而是 <em>跟人類一樣</em> 直接處理這些符號。<span><label for="sn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="marginnote"> 我最早是在 SICP 一書看見這個作法，可以忍受很多括號的讀者可以去看一下 <a href="https://mitpress.mit.edu/sicp/full-text/book/book-Z-H-16.html#%_sec_2.3.2">§2.3.2</a> 的介紹。<br />
<br />
</span></span> 在修習微積分課程時，一定會提到如何利用 <a href="https://en.wikipedia.org/wiki/Differentiation_rules">Differentiation rules</a> 來求出導數，像是:</p>
<ul>
<li><span class="math inline">\((a \cdot f)&#39; = af&#39;\)</span></li>
<li><span class="math inline">\((f + g)&#39; = f&#39; + g&#39;\)</span></li>
<li><span class="math inline">\((f(x) \cdot g(x))&#39; = f&#39;(x)g(x) + f(x)g&#39;(x)\)</span></li>
<li><span class="math inline">\((f(g(x)))&#39; = f&#39;(g(x)) g&#39;(x)\)</span></li>
</ul>
<p>而 Symbolic Differential 就是採用跟人類一樣的方法，對著 expression tree 不斷的做 pattern matching 跟 rewrite。這個方法很好實作，只需要照著 Differential rules 改寫便是，而且可以得到<strong>真正的</strong><span><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Exact Derivative，也就是跟手動推導的結果一致，Numerical Differential 因為浮點數的特性，會產生誤差而無法算出真正的導數/偏導數<br />
<br />
</span></span>導數/偏導數。</p>
<p>但是這個方法最大的問題是在改寫的過程中，會出現很多冗於的式子： <span class="math inline">\(x \times 1\)</span>, <span class="math inline">\(x + 0\)</span> 之類的，因此很容易產生出巨大的（沒效率）的結果。<span><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">“Automatic Differentiation in Machine Learning: a Survey” 一文中舉例到： <span class="math display">\[f(x) = 64x(1 - x)(1 - 2x)^2(1 - 8x + 8x^2)^2\]</span> 在沒有化簡的情況做符號微分會得到 <span class="math display">\[\begin{gathered}
  128x(1 - x)(-8 + 16x)(1 - 2x)^2(1 - 8x+8x^2) \\
  + 64(1-x)(1-2x)^2 (1-8x+ 8x^2)^2\\
  - 64x(1-2x)^2 (1-8x+8x^2)^2 \\
  - 256x(1 - x)(1 - 2x)(1 - 8x + 8x^2)^2
  \end{gathered} \]</span> 這條很複雜的式子，但是經過化簡則只剩下 <span class="math display">\[\begin{gathered}
  64(1 - 42x + 504x^2 - 2640x^3  \\
  + 7040x^4 -9984x^5 + 7168x^6 -2048x^7)
  \end{gathered}\]</span>，而隨著原式<span class="math inline">\(f\)</span>的複雜度上升，沒做好簡化的符號微分會膨脹非常快。<br />
<br />
</span></span></p>
<p>以 Python 來說，SymPy 就實作了符號積分，可以看到 <code>diff</code> 會產出一個新的運算式。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="im">from</span> sympy <span class="im">import</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">x <span class="op">=</span> var(<span class="st">&#39;x&#39;</span>)</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">expr <span class="op">=</span> diff(sin(x)) <span class="co"># cos(x)</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">expr.evalf(subs<span class="op">=</span>{<span class="st">&#39;x&#39;</span>: pi}) <span class="co"># -1.000000000000</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"></a>
<a class="sourceLine" id="cb2-6" data-line-number="6">expr2 <span class="op">=</span> diff(sin(x)<span class="op">+</span> x <span class="op">*</span> y, x)</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">expr2.evalf(subs<span class="op">=</span>{<span class="st">&#39;x&#39;</span>: <span class="dv">3</span>, <span class="st">&#39;y&#39;</span>: <span class="dv">2</span>}) <span class="co"># 1.0100075033995546</span></a></code></pre></div>
<!--

左圖為$f(x, y) = sin(x) + x*y$的示意圖，右圖為以符號計算$\frac{df}{dx}$的示意圖：


~~~~
        f(x, y)    |         dfdx
          +-+      |         +-+
          |+|      |         |+|
          +-+      |         +-+
         /  \      |        /  \
        /    \     |       /    \
    +---+     +-+  |   +---+     +-+
    |sin|     |×|  |   |cos|     |0|
    +---+     +-+  |   +---+     +-+
      |       /|   |     |
      | _____/ |   |     |
      |/       |   |     |
     +-+      +-+  |    +-+
     |x|      |y|  |    |x|
     +-+      +-+  |    +-+
~~~~~
-->
</section>
<section id="forward-mode-automatic-differential" class="level1">
<h1>Forward-Mode Automatic Differential</h1>
<p>自動微分（AD）的名稱取得很容易讓人誤解，其實就是應用 chain rule 跟一些語言特性或是原始碼轉換工具，來達成不用手寫導數的程式，而可以在計算函數的同時（也就是 overhead 不大）得出<strong>真正的</strong>導數/偏導數。</p>
<p>自動微分根據計算的順序，可以分為 Forward Mode 跟 Reverse Mode，這次先介紹比較簡單的 Forward Mode。</p>
<p><span><label for="sn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="marginnote">{-} Forward Mode AD 將一個函數 <span class="math inline">\(y = f(... , x, ...) = (w_m \circ w_{m-1} ... \circ w_1)(..., x, ...)\)</span> 對 <span class="math inline">\(x\)</span> 的微分，拆解成 <span class="math display">\[
  \begin{aligned}
  \frac {\partial y}{\partial x} =&amp;\frac {\partial y}{\partial w_{m-1}}{\frac {\partial w_{m-1}}{\partial x}} \\
  =&amp;\frac {\partial y}{\partial w_{m-1}}\left({\frac {\partial w_{m-1}}{\partial w_{m-2}}}{\frac {\partial w_{m-2}}{\partial x}}\right) \\
  =&amp;\frac {\partial y}{\partial w_{m-1}}\left({\frac {\partial w_{m-1}}{\partial w_{m-2}}}\left({\frac {\partial w_{m-2}}{\partial w_{m-3}}}{\frac {\partial w_{m-3}}{\partial x}}\right)\right) \\
  =&amp;{\frac {\partial y}{\partial w_{m-1}}} \left( \frac {\partial w_{m-1}}{\partial w_{m-2}} \cdots
  \left(\frac {\partial w_{2}}{\partial w_{1}}  \frac {\partial w_{1}}{\partial x} \right)\right)\\
  \end{aligned}
  \]</span> 然後從 <span class="math inline">\(\frac{\partial x}{\partial x} = 1\)</span> 開始， 由 <span class="math inline">\(\frac{\partial w_1}{\partial x}\)</span> 開始計算到 <span class="math inline">\(\frac{\partial w_m}{x}\)</span>（由內而外）。 <br />
<br />
</span></span></p>
<p>這邊繼續使用上面的 <span class="math inline">\(Y = f(x, y) = sin(x) + x × y\)</span> 的例子，</p>
<p>我們可以把 <span class="math inline">\(Y\)</span> 拆解成下圖（左）</p>
<p><span class="math display">\[
\begin{array}{rl|rl}
  Y &amp; =  w_5       &amp; \dot{Y} &amp; = \dot{w_5} \\
w_5 &amp; =  w_3 + w_4 &amp; \dot{w_5} &amp; = \dot{w_3} + \dot{w_4} \\
w_4 &amp; =  w_1 × w_2 &amp; \dot{w_4} &amp; = w_2 × \dot{w_1} + w_1 × \dot{w_2}\\
w_3 &amp; =  sin(w_1)  &amp; \dot{w_3} &amp; = cos(w_1) \\
w_2 &amp; =  y         &amp; \dot{w_2} &amp; = 0\\
w_1 &amp; =  x         &amp; \dot{w_1} &amp; = 1\\
\end{array}
\]</span></p>
<p>然後我們就可以從 <span class="math inline">\(\frac{\partial x}{\partial x} = 1, \frac{\partial y}{\partial x} = 0\)</span> 開始，求出 <span class="math inline">\(\frac{\partial w_i}{\partial x} \big \vert_{i = 1 \cdots 5}\)</span>，最後求出<span class="math inline">\(\frac{\partial Y}{\partial x}\)</span> （如上圖（右），這邊用 <span class="math inline">\(\dot{X}\)</span> 來表示 <span class="math inline">\(\frac{\partial X}{\partial x}\)</span>）。</p>
<p>從計算順序上，Forward-Mode AD 跟原先函數是一樣的，所以被稱為 Forward Mode。</p>
<section id="實作-1" class="level2">
<h2>實作</h2>
<p>一個常見的實作方法是定義一個新的資料結構 Dual Number，跟利用 operator overloading 來實作其算術系統。下面使用 Python 舉例<span><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">較完整的 Dual Number 定義可以在 <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Automatic_differentiation_using_dual_numbers">Wikipedia</a> 找到。如同文中寫到，我們可以使用 <span class="math inline">\(g(\langle u,u&#39; \rangle , \langle v,v&#39; \rangle )  = \langle g(u,v) , g_u(u,v) u&#39; + g_v(u,v) v&#39; \rangle\)</span> 來定義這些 primitive functions(<span class="math inline">\(sin, cos, \cdots\)</span>)。<br />
<br />
</span></span><span><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">為了可以寫出像是 <code class="sourceCode python"><span class="dv">4</span> <span class="op">*</span> Dual(<span class="dv">1</span>, <span class="dv">1</span>)</code> 這樣混合著 <code>float</code> 跟 <code>Dual</code> 的程式，可以參考 <a href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types">Python Reference §3.3.7</a> 使用 <code>__radd__</code> 系列的 method 來實作。<br />
<br />
</span></span>：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">from</span> collections <span class="im">import</span> namedtuple</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> math</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="kw">class</span> Dual(namedtuple(<span class="st">&quot;Dual&quot;</span>, [<span class="st">&quot;x&quot;</span>, <span class="st">&quot;dx&quot;</span>])):</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    <span class="co">&#39;&#39;&#39;dx 追蹤 x 的變化對當下輸出的影響&#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">        <span class="cf">return</span> Dual(<span class="va">self</span>.x <span class="op">+</span> other.x, <span class="va">self</span>.dx <span class="op">+</span> other.dx)</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">        <span class="cf">return</span> Dual(<span class="va">self</span>.x <span class="op">*</span> other.x, other.x <span class="op">*</span> <span class="va">self</span>.dx <span class="op">+</span> <span class="va">self</span>.x <span class="op">*</span> other.dx)</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">    <span class="co"># 下略</span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11"></a>
<a class="sourceLine" id="cb3-12" data-line-number="12"><span class="kw">def</span> sin(dual):</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="co"># inline chain rule here</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14">    <span class="cf">return</span> Dual(math.sin(dual.x), dual.dx <span class="op">*</span> math.cos(dual.x))</a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co"># 這邊省略其他函數</span></a></code></pre></div>
<p>舉例來說，我們可以透過控制<code>Dual(x, dx)</code>的<code>dx</code>來分別計算 <span class="math inline">\(\frac{\partial f}{\partial x}\)</span> 跟 <span class="math inline">\(\frac{\partial f}{\partial y}\)</span></p>
<p><span><label for="sn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="marginnote"> <a href="https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/ForwardAccumulationAutomaticDifferentiation.png/512px-ForwardAccumulationAutomaticDifferentiation.png" /></a> 這是 Wikipedia 上面對 <span class="math inline">\(Y = f(x,y) = sin(x) + x × y\)</span> 做 forward-mode AD 的示意圖。<br />
By Berland at English Wikipedia [Public domain], via Wikimedia Commons<br />
<br />
</span></span></p>
<p><span class="math display">\[f(x,y) = sin(x) + x*y\]</span></p>
<p>（<code class="sourceCode python">Dual(x, <span class="dv">1</span>)</code> 對應到 <span class="math inline">\(\frac{dx}{dx} = 1\)</span>；<code class="sourceCode python">Dual(c, <span class="dv">0</span>)</code> 對應到 <span class="math inline">\(\frac{dc}{dx} = 0\)</span>）</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># 要計算偏導數的函數可以用本來的寫法，而不用自幹，有沒有感覺到「自動」？</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">def</span> f(x, y):</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">  <span class="cf">return</span> sin(x) <span class="op">+</span> x<span class="op">*</span>y</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">f(Dual(<span class="dv">3</span>, <span class="dv">1</span>), Dual(<span class="dv">2</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb4-6" data-line-number="6"><span class="co"># Dual(x=6.141120008059867, dx=1.0100075033995546)</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7"></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">f(Dual(<span class="dv">3</span>, <span class="dv">0</span>), Dual(<span class="dv">2</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb4-9" data-line-number="9"><span class="co"># Dual(x=6.141120008059867, dx=3.0)</span></a></code></pre></div>
<p>這是怎麼辦到的？我們可以用類似 Equational Reasoning<span><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote">Equational Reasoning 就像數學證明一樣，比較常用在 Pure Functional 的程式語言上，不過既然這個例子也幾乎是 Pure Functional 的，在這邊就使用這個技巧來說明。<br />
<br />
</span></span> 的方式來展開：</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">(  f(Dual(<span class="dv">3</span>, <span class="dv">1</span>), Dual(<span class="dv">2</span>,<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="op">==</span> sin(Dual(<span class="dv">3</span>, <span class="dv">1</span>)) <span class="op">+</span> Dual(<span class="dv">3</span>, <span class="dv">1</span>) <span class="op">*</span> Dual(<span class="dv">2</span>,<span class="dv">0</span>)                        <span class="co"># definition of f</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="op">==</span> Dual(math.sin(<span class="dv">3</span>), <span class="dv">1</span> <span class="op">*</span> math.cos(<span class="dv">3</span>)) <span class="op">+</span> Dual(<span class="dv">3</span>, <span class="dv">1</span>) <span class="op">*</span> Dual(<span class="dv">2</span>,<span class="dv">0</span>)     <span class="co"># definition of sin</span></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="op">==</span> Dual(math.sin(<span class="dv">3</span>), <span class="dv">1</span> <span class="op">*</span> math.cos(<span class="dv">3</span>)) <span class="op">+</span> Dual(<span class="dv">3</span> <span class="op">*</span> <span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">0</span>) <span class="co"># definition of __mul__</span></a>
<a class="sourceLine" id="cb5-5" data-line-number="5"><span class="op">==</span> Dual(math.sin(<span class="dv">3</span>) <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">2</span>, <span class="dv">1</span> <span class="op">*</span> math.cos(<span class="dv">3</span>) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">0</span>)      <span class="co"># definition of __add__</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6"><span class="op">==</span> Dual(x<span class="op">=</span><span class="fl">6.141120008059867</span>, dx<span class="op">=</span><span class="fl">1.0100075033995546</span>)  )</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"><span class="co"># returns True since this is a valid python expression.</span></a></code></pre></div>
<p>順帶一提，我們不需要額外實作 Chain Rule，因為它已經包含在 Dual Number 的算術系統內了。</p>
<p><span><label for="sn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="marginnote"> 可以看到第 3 行的 <code>dx</code> 正好等於應用 Chain Rule 求出的 <span class="math inline">\(\frac{\partial sin(sin(x))}{\partial x} = cos(x) × cos(sin(x))\)</span>。<br />
這是因為在 <code>sin(dual)</code> 的定義中 <code class="sourceCode python">Dual(math.sin(dual.x), dual.dx <span class="op">*</span> math.cos(dual.x))</code> 裡面就已經 <em>inline</em> chain rule 了<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">(  sin(sin(Dual(<span class="dv">1</span>, <span class="dv">1</span>)))</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="op">==</span> sin(Dual(math.sin(<span class="dv">1</span>), <span class="dv">1</span> <span class="op">*</span> math.cos(<span class="dv">1</span>)))                          <span class="co"># definition of sin</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="op">==</span> Dual(math.sin(math.sin(<span class="dv">1</span>)), math.cos(<span class="dv">1</span>) <span class="op">*</span> math.cos(math.sin(<span class="dv">1</span>))) <span class="co"># definition of sin</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="op">==</span> Dual(x<span class="op">=</span><span class="fl">0.7456241416655579</span>, dx<span class="op">=</span><span class="fl">0.36003948908962097</span>) )</a></code></pre></div>
<!--

左圖為$f = sin(x) + x * y$的示意圖，右邊為以 Dual Number 實作的示意圖：

~~~~
        f(x, y)    |       f(x, y); dfdx                      f(3,2); dfdx
          +-+      |          +---+---+                       +-----+-----+
          |+|      |          | + | + |                       |6.14 | 1.01|
          +-+      |          +---+---+                       +-----+-----+
         /  \      |          /       \                        /       \
        /    \     |         /         \                      /         \
    +---+     +-+  |  +------+------+  +--+----------+   +----+-----+   +--+--+
    |sin|     |×|  |  |sin(a)|cos(a)|  |× | a*db+b*db|   |0.14|-0.98|   | 6| 2|
    +---+     +-+  |  +------+------+  +--+----------+   +----+-----+   +--+--+
      |       /|   |        |           / |                   |          / |
      | _____/ |   |        |  ________/  |                   |  _______/  |
      |/       |   |        | /           |                   | /          |
     +-+      +-+  |     +--+--+       +--+--+             +--+--+      +--+--+
     |x|      |y|  |     |x | 1|       |y | 0|             |3 | 1|      |2 | 0|
     +-+      +-+  |     +--+--+       +--+--+             +--+--+      +--+--+
~~~~

-->
<p>這個方法的問題是每一個參數<span class="math inline">\(x_i\)</span>都必須算過一次，類神經的梯度來說需要<span class="math inline">\(O(n)\)</span>的時間才能算完（<span class="math inline">\(n\)</span>是參數的個數），並不是很適合類神經網路計算梯度（因為<span class="math inline">\(n\)</span>通常都不小）。</p>
</section>
</section>
<section id="小結" class="level1">
<h1>小結</h1>
<p>本文介紹了</p>
<ul>
<li>Numerical Differential，用近似的方法計算，其優點是實作最簡單，缺點是計算誤差大到不夠實用</li>
<li>Symbolic Differential，其實就是把人類計算微分的過程用電腦實作，可以得到正確的數值，但是會遇到算式膨脹的問題</li>
<li>Forward-Mode Automatic Differential，應用了 chain rule，在計算的過程也可以簡單的算出正確的偏導數，但是不適合計算梯度。</li>
</ul>
<p>下一篇文章會介紹 Reverse-Mode Automatic Differential，是 Forward-Mode 相反方向的版本，同時也是 back propagation 的 general 版本。</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation on Wikipedia</a></li>
<li><a href="https://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2">Backpropogation is Just Steepest Descent with Automatic Differentiation</a></li>
<li><a href="https://arxiv.org/pdf/1502.05767.pdf">Automatic Differentiation in Machine Learning: a Survey</a></li>
<li><a href="https://arxiv.org/pdf/1404.7456.pdf">Automatic Differentiation of Algorithms for Machine Learning</a></li>
</ul>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="example-logistic-regression" class="level2">
<h2>Example: Logistic Regression</h2>
<p>這邊用挑戰者號的O型環失效資料<span><label for="sn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-11" class="margin-toggle"/><span class="marginnote"> 選這份資料是因為我最近在讀 “Probabilistic Programming &amp; Bayesian Methods for Hackers” 剛好介紹到這一份<a href="https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv">資料集</a>。<br />
<br />
</span></span>作為範例，以上述的 Forward-Mode AD<span><label for="sn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-12" class="margin-toggle"/><span class="marginnote"> <strong>注意:</strong> 這邊的 <code>forwardad</code> 已經有實作 <code>__radd__</code>, <code>exp</code>, <code>log</code>…等滿足下面程式所需的最小需求。<br />
<br />
</span></span> 實作 Gradient Descent 來進行 Logistic Regression。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="im">from</span> forwardad <span class="im">import</span> <span class="op">*</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="im">from</span> io <span class="im">import</span> StringIO</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5">data <span class="op">=</span> StringIO(<span class="st">&#39;Date,Temperature,Damage Incident</span><span class="ch">\n</span><span class="st">04/12/1981,66,0.0</span><span class="ch">\n</span><span class="st">11/12/1981,70,1.0</span><span class="ch">\n</span><span class="st">3/22/82,69,0.0</span><span class="ch">\n</span><span class="st">6/27/82,80,</span><span class="ch">\n</span><span class="st">01/11/1982,68,0.0</span><span class="ch">\n</span><span class="st">04/04/1983,67,0.0</span><span class="ch">\n</span><span class="st">6/18/83,72,0.0</span><span class="ch">\n</span><span class="st">8/30/83,73,0.0</span><span class="ch">\n</span><span class="st">11/28/83,70,0.0</span><span class="ch">\n</span><span class="st">02/03/1984,57,1.0</span><span class="ch">\n</span><span class="st">04/06/1984,63,1.0</span><span class="ch">\n</span><span class="st">8/30/84,70,1.0</span><span class="ch">\n</span><span class="st">10/05/1984,78,0.0</span><span class="ch">\n</span><span class="st">11/08/1984,67,0.0</span><span class="ch">\n</span><span class="st">1/24/85,53,1.0</span><span class="ch">\n</span><span class="st">04/12/1985,67,0.0</span><span class="ch">\n</span><span class="st">4/29/85,75,0.0</span><span class="ch">\n</span><span class="st">6/17/85,70,0.0</span><span class="ch">\n</span><span class="st">7/29/85,81,0.0</span><span class="ch">\n</span><span class="st">8/27/85,76,0.0</span><span class="ch">\n</span><span class="st">10/03/1985,79,0.0</span><span class="ch">\n</span><span class="st">10/30/85,75,1.0</span><span class="ch">\n</span><span class="st">11/26/85,76,0.0</span><span class="ch">\n</span><span class="st">01/12/1986,58,1.0</span><span class="ch">\n</span><span class="st">1/28/86,31,1.0</span><span class="ch">\n</span><span class="st">&#39;</span>)</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">df <span class="op">=</span> pd.read_csv(data).dropna()</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">X <span class="op">=</span> df[<span class="st">&#39;Temperature&#39;</span>].tolist()</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">Y <span class="op">=</span> df[<span class="st">&#39;Damage Incident&#39;</span>].tolist()</a></code></pre></div>
<p>這邊使用 logistic function 做模型，binary cross-entropy 作為 loss function。</p>
<p><span class="math display">\[
\begin{gathered}
f(x) = \frac{1}{1+ exp(\beta x + \alpha)} \\
Cost(y, \hat y) = -\frac{1}{N}\sum_{i=1}^N \bigg[ y_i \log(\hat{y}_i)+(1-y_i) \log(1-\hat{y}_i) \bigg]
\end{gathered}
\]</span> <span><label for="sn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-13" class="margin-toggle"/><span class="marginnote"> 這邊可以看到，寫出來的程式幾乎沒有為了計算微分而改變，而是照著本來的定義在寫。<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">def</span> f(alpha, beta, x):</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> exp(beta <span class="op">*</span> x <span class="op">+</span> alpha))</a>
<a class="sourceLine" id="cb8-3" data-line-number="3"></a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="kw">def</span> cross_entroy(Y, Y_):</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">    loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb8-6" data-line-number="6">    <span class="cf">for</span> y, y_ <span class="kw">in</span> <span class="bu">zip</span>(Y, Y_):</a>
<a class="sourceLine" id="cb8-7" data-line-number="7">        loss <span class="op">-=</span> y <span class="op">*</span> log(y_) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> log(<span class="dv">1</span> <span class="op">-</span> y_)</a>
<a class="sourceLine" id="cb8-8" data-line-number="8">    <span class="cf">return</span> loss <span class="op">/</span> <span class="bu">len</span>(Y)</a></code></pre></div>
<p>最後使用 Gradient Descent 來找出 <span class="math inline">\(\alpha, \beta\)</span>。</p>
<p><span><label for="sn-14" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-14" class="margin-toggle"/><span class="marginnote"> <code>alpha</code>, <code>beta</code>, <code>lr</code>, <code>n_epoch</code> 這些參數都是可以設定的。<br />
<br />
</span></span></p>
<p><span><label for="sn-15" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-15" class="margin-toggle"/><span class="marginnote"> 受限於 Forward-Mode AD 先天上的限制，<span class="math inline">\(\alpha\)</span> 跟 <span class="math inline">\(\beta\)</span> 的微分需要分開計算。<br />
<br />
</span></span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python example"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">alpha <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2">beta  <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb9-3" data-line-number="3">lr <span class="op">=</span> <span class="fl">0.005</span></a>
<a class="sourceLine" id="cb9-4" data-line-number="4">n_epoch <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb9-5" data-line-number="5"></a>
<a class="sourceLine" id="cb9-6" data-line-number="6"><span class="co"># 為了避免 log(0) ，這邊先置換成 0 ≈ 1e-18</span></a>
<a class="sourceLine" id="cb9-7" data-line-number="7">Y <span class="op">=</span> [<span class="fl">1e-18</span> <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> x <span class="cf">for</span> x <span class="kw">in</span> Y]</a>
<a class="sourceLine" id="cb9-8" data-line-number="8"></a>
<a class="sourceLine" id="cb9-9" data-line-number="9"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epoch):</a>
<a class="sourceLine" id="cb9-10" data-line-number="10">    Y_alpha <span class="op">=</span> [f(Dual(alpha, <span class="dv">1</span>), beta, x) <span class="cf">for</span> x <span class="kw">in</span> X]</a>
<a class="sourceLine" id="cb9-11" data-line-number="11">    Y_beta  <span class="op">=</span> [f(alpha, Dual(beta, <span class="dv">1</span>), x) <span class="cf">for</span> x <span class="kw">in</span> X]</a>
<a class="sourceLine" id="cb9-12" data-line-number="12">    loss_alpha <span class="op">=</span> cross_entroy(Y, Y_alpha)</a>
<a class="sourceLine" id="cb9-13" data-line-number="13">    loss_beta  <span class="op">=</span> cross_entroy(Y, Y_beta)</a>
<a class="sourceLine" id="cb9-14" data-line-number="14">    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb9-15" data-line-number="15">        <span class="bu">print</span>(<span class="ss">f&#39;</span><span class="sc">{</span>epoch<span class="sc">: 5d}</span><span class="ss"> loss </span><span class="sc">{</span>loss_alpha<span class="sc">.x}</span><span class="ss"> alpha </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss"> beta </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss">&#39;</span>)</a>
<a class="sourceLine" id="cb9-16" data-line-number="16">    alpha <span class="op">-=</span> lr <span class="op">*</span> loss_alpha.dx</a>
<a class="sourceLine" id="cb9-17" data-line-number="17">    beta  <span class="op">-=</span> lr <span class="op">*</span> loss_beta.dx</a></code></pre></div>
<p>這邊是上面程式執行的結果（重新排版過），可以看到 Learning Rate 設定的有點太大了。有興趣的讀者可以試著自己實作看看。</p>
<pre><code>    0 loss 0.693147180559945  alpha  0                   beta  0
  100 loss 0.966931426916483  alpha -0.0201008838271800  beta  0.0465418617995232
  200 loss 1.114664696718874  alpha -0.0417346760460772  beta -0.0170795861942109
  300 loss 3.558155059566457  alpha -0.0595466871292758  beta  0.1800165252346373
  400 loss 0.961973323155755  alpha -0.0821799396148940  beta  0.0472951650365717
  500 loss 1.110644944663376  alpha -0.1037522632990092  beta -0.0161308641984885
  600 loss 3.555985812065263  alpha -0.1215047609258659  beta  0.1809462252622945
  700 loss 0.957097575658038  alpha -0.1440822016889158  beta  0.0480494092224557
  800 loss 1.106712443731962  alpha -0.1655931706446121  beta -0.0151865701942648
  900 loss 3.553772437842705  alpha -0.1832862200585766  beta  0.1818707335807102</code></pre>
</section>
</section>]]></description>
    <pubDate>Tue, 09 Jan 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2017-12-17-autodiff.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>更換 blog 主題以迎接 2018 年</title>
    <link>https://op8867555.github.io/posts/2018-01-05-new-style.html</link>
    <description><![CDATA[<section>
<p>過了 2017，我決定換掉之前用 Google 的 <a href="https://getmdl.io/">Material Design Lite</a> 手刻的 css，改用了 <a href="https://github.com/edwardtufte/tufte-css">Tufte CSS</a> <span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">正確來說是它的 pandoc 版 fork “<a href="https://github.com/jez/tufte-pandoc-css">tufte-pandoc-css</a>”<br />
<br />
</span></span> 這套以 Edward Tufte 的書籍及手稿<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">我沒看過，但是就是樣利用兩欄設計，把 foot note 改作 side note 的風格。<br />
<br />
</span></span>的風格為基礎的主題。</p>
<p>這邊特地截了兩者的截圖以作為紀念（不過其實<code>git checkout</code>一下也行）</p>
<p><span><label for="sn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="marginnote"> 之前的主題走 Material Design 的卡片風格，以無襯線的黑體為主。 <a href="https://i.imgur.com/TQjq0Ju.png"><img src="https://i.imgur.com/TQjq0Ju.png" /></a><br />
<br />
</span></span></p>
<p><a href="https://i.imgur.com/e5mGJpO.png"><img src="https://i.imgur.com/e5mGJpO.png" /></a></p>
<p>除此之外，我還做了：</p>
<ul>
<li>改用 <a href="https://khan.github.io/KaTeX/"><span class="math inline">\(\KaTeX\)</span></a> 這個前端渲染引擎來呈現 LaTeX 的運算式，相較於之前 pandoc 的基本款實作支援更多語法，而且渲染結果也好看了不少。</li>
<li>使用 pandoc 的 <code>eastAsianLineBreakFilter</code> 來改進中文斷行的呈現。</li>
</ul>
</section>]]></description>
    <pubDate>Fri, 05 Jan 2018 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2018-01-05-new-style.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>
<item>
    <title>筆記：幾個 CLI 實用工具: ag, fzf, fd, ripgrep</title>
    <link>https://op8867555.github.io/posts/2017-11-27-cli-tools-ag-rg-fzf-fd.html</link>
    <description><![CDATA[<p>筆記最近用的幾個 CLI 工具。</p>
<section id="the-silver-searcher" class="level1">
<h1>The Silver Searcher</h1>
<p><a href="https://github.com/ggreer/the_silver_searcher">The Silver Searcher</a> 是一個類似 ack 的搜尋工具，也就是找出出現某字串的所有檔案。</p>
<ul>
<li>飛天快</li>
<li>預設排除 <code>.gitignore</code> <code>.hgignore</code> 裡的 pattern</li>
<li>其他不想要的 pattern 可以寫在 <code>.ignore</code></li>
<li>用 c 實作</li>
<li>直到 <span class="citation" data-cites="caasih">@caasih</span> 提到之前我都沒發現執行檔名稱 <code>ag</code> 是水銀的元素符號</li>
</ul>
</section>
<section id="ripgrep" class="level1">
<h1>ripgrep</h1>
<p><a href="https://github.com/BurntSushi/ripgrep">Ripgrep</a> 跟前述的 The Silver Searcher 功能差不多</p>
<ul>
<li>但是比飛天快還快</li>
<li>對於 ignore 檔案的實作更正確，ag 會多吐一些不應該出現的結果</li>
<li>regexp 的功能比較少（比方說我從來沒用過的 lookaround 跟 backreference 就沒有實作）</li>
<li>用 rust 實作</li>
</ul>
</section>
<section id="fd" class="level1">
<h1>fd</h1>
<p><a href="https://github.com/sharkdp/fd">fd</a> 是一個類似 find 的檔案搜尋工具</p>
<ul>
<li>也會預設排除 <code>.gitignore</code></li>
<li>預設排除隱藏檔案</li>
<li>也是飛天快</li>
<li>也是 rust 實作</li>
</ul>
<section id="範例" class="level2">
<h2>範例</h2>
<ul>
<li><p>用 regexp 搜尋</p>
<pre><code>fd &#39;[0-9]\.jpg$&#39;</code></pre></li>
<li><p>搜尋隱藏檔案跟被 ignore 的檔案</p>
<pre><code>fd -H -I &#39;[0-9]\.jpg$&#39;</code></pre></li>
<li><p>搜尋特定副檔名</p>
<pre><code>fd -e jpg</code></pre></li>
<li><p>對搜尋結果下指令<strong>平行執行</strong></p>
<pre><code>fd -e jpg -x convert {} {.}.png</code></pre>
<table>
<thead>
<tr class="header">
<th>variable</th>
<th style="text-align: left;">example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>{}</td>
<td style="text-align: left;">documents/images/party.jpg</td>
</tr>
<tr class="even">
<td>{.}</td>
<td style="text-align: left;">documents/images/party</td>
</tr>
<tr class="odd">
<td>{/}</td>
<td style="text-align: left;">party.jpg</td>
</tr>
<tr class="even">
<td>{//}</td>
<td style="text-align: left;">documents/images</td>
</tr>
<tr class="odd">
<td>{/.}</td>
<td style="text-align: left;">party</td>
</tr>
</tbody>
</table></li>
</ul>
</section>
</section>
<section id="fzf" class="level1">
<h1>fzf</h1>
<p><a href="https://github.com/junegunn/fzf">fzf</a> 是一個 fuzzy finder，就是 cmd+T 或是 ctrl+p 或是任何一個 文字編輯器/IDE 裡面不用打全名就可以找檔案的那個功能</p>
<ul>
<li>用 go 寫的</li>
<li><p>預設用 find ，所以很慢，用環境變數可以換用 fd</p>
<p>export FZF_DEFAULT_COMMAND=‘fd –type f –follow –exclude .git’</p></li>
<li><p>ctrl+r 來搜尋歷史</p>
<blockquote class="imgur-embed-pub" lang="en" data-id="IwjCK2l">
<a href="//imgur.com/IwjCK2l">View post on imgur.com</a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script></li>
<li><p><code>**&lt;tab&gt;</code> 可以選取多個檔案</p>
<blockquote class="imgur-embed-pub" lang="en" data-id="ocxqb0C">
<a href="//imgur.com/ocxqb0C">View post on imgur.com</a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script></li>
<li><p>ctrl+t 來搜尋檔案</p>
<blockquote class="imgur-embed-pub" lang="en" data-id="SfiGWjD">
<a href="//imgur.com/SfiGWjD">View post on imgur.com</a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script></li>
</ul>
</section>]]></description>
    <pubDate>Mon, 27 Nov 2017 00:00:00 UT</pubDate>
    <guid>https://op8867555.github.io/posts/2017-11-27-cli-tools-ag-rg-fzf-fd.html</guid>
    <dc:creator>Alex Lu</dc:creator>
</item>

    </channel>
</rss>
